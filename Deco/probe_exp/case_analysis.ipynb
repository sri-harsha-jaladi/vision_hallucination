{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX, # IMAGE_TOKEN_INDEX = -200\n",
    "    DEFAULT_IMAGE_TOKEN, # DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "    DEFAULT_IM_START_TOKEN, # DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "    DEFAULT_IM_END_TOKEN, # DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "    IMAGE_PLACEHOLDER, # IMAGE_PLACEHOLDER = \"<image-placeholder>\"\n",
    ")\n",
    "\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    "    KeywordsStoppingCriteria,\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "\n",
    "def image_parser(args):\n",
    "    out = args.image_file.split(args.sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "model_name = get_model_name_from_path(\"llava-v1.5-7b\")\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        \"llava-v1.5-7b\", None, model_name, device=\"cuda:0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "Describe the image in detail. ASSISTANT:\n",
      "================\n",
      "torch.Size([1, 3, 336, 336])\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "Describe the image in detail. ASSISTANT: The image showcases a street corner in a city, featuring street signs mounted on a street light post. The signs provide directions to various locations, including the local library and a nearby market. The sky above the street corner is blue, indicating a clear day.\n",
      "\n",
      "In the background, there is a car\n",
      "torch.float16\n",
      "using dola greedy search\n"
     ]
    }
   ],
   "source": [
    "query = \"Describe the image in detail.\"\n",
    "# query = \"Generate a caption for this image.\"\n",
    "# query = \"Please describe this image in detail.\"\n",
    "image_file = \"case3.jpg\"\n",
    "probe_prompt = \"The image showcases a street corner in a city, featuring street signs mounted on a street light post. The signs provide directions to various locations, including the local library and a nearby market. The sky above the street corner is blue, indicating a clear day.\\n\\nIn the background, there is a\"\n",
    "args_dict = {\n",
    "                \"model_path\": \"llava-v1.5-7b\",\n",
    "                \"model_base\": None,\n",
    "                \"model_name\": get_model_name_from_path(\"llava-v1.5-7b\"),\n",
    "                \"query\": query,\n",
    "                \"conv_mode\": None,\n",
    "                \"image_file\": image_file,\n",
    "                \"sep\": \",\",\n",
    "                \"temperature\": -1,\n",
    "                \"top_p\": None, \n",
    "                \"num_beams\": 1,\n",
    "                \"max_new_tokens\": 2048\n",
    "            }\n",
    "args = type('Args', (), args_dict)()\n",
    "\n",
    "# Model\n",
    "# disable_torch_init()\n",
    "qs = args.query\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "if IMAGE_PLACEHOLDER in qs:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "    else:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = image_token_se + \"\\n\" + qs\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "if \"llama-2\" in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "if args.conv_mode is not None and conv_mode != args.conv_mode:\n",
    "    print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, args.conv_mode, args.conv_mode\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    args.conv_mode = conv_mode\n",
    "\n",
    "conv = conv_templates[args.conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "    \n",
    "image_files = image_parser(args)\n",
    "images = load_images(image_files)\n",
    "images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor, # clip\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda(0)\n",
    "    )\n",
    "\n",
    "input = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\n{} ASSISTANT: \".format(query) + probe_prompt\n",
    "\n",
    "input_ids= (\n",
    "        tokenizer_image_token(input, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda(0)\n",
    "    )\n",
    "\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "        output_dict = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            do_sample=True if args.temperature > 0 else False,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p,\n",
    "            num_beams=args.num_beams,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            use_cache=True,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_hidden_states=True,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "            dola_decoding=True,\n",
    "            alpha = 0.6,\n",
    "            threshold_top_p = 0.9,\n",
    "            threshold_top_k = 10,\n",
    "            min_candidate_tokens = None,\n",
    "            early_exit_layers=[i for i in range(15, 25, 1)],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.2832e-01, 4.4482e-01, 2.2141e-02, 3.1910e-03, 4.3178e-04, 3.2592e-04,\n",
       "          1.6654e-04, 1.1623e-04, 9.1910e-05, 6.7234e-05, 5.7518e-05, 4.3452e-05,\n",
       "          2.8491e-05, 2.0862e-05, 1.5974e-05, 8.7023e-06, 8.5235e-06, 7.5698e-06,\n",
       "          7.3314e-06, 6.9737e-06]], device='cuda:0', dtype=torch.float16),\n",
       " tensor([[ 1939,  3869,  1670,   450,  5806,   512,  8512, 16564,   739,  1094,\n",
       "            306,   319, 14832,   910,  4001,  3118,  2398,  2216, 11511,  8669]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "with torch.no_grad():\n",
    "    mapping_layer = model.get_output_embeddings()\n",
    "    out = mapping_layer(output_dict.hidden_states[0][-1][:,-1,:])\n",
    "\n",
    "next_probs = F.softmax(out, dim=-1)\n",
    "top_k_probs, top_k_ids = torch.topk(next_probs, dim=-1, k=20)\n",
    "\n",
    "top_k_probs, top_k_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁No',\n",
       " '▁Yes',\n",
       " '▁There',\n",
       " '▁The',\n",
       " '▁While',\n",
       " '▁In',\n",
       " '▁Although',\n",
       " '▁Based',\n",
       " '▁It',\n",
       " '▁As',\n",
       " '▁I',\n",
       " '▁A',\n",
       " '▁Though',\n",
       " '▁This',\n",
       " '▁Since',\n",
       " '▁One',\n",
       " '▁However',\n",
       " '▁Not',\n",
       " '▁Unfortunately',\n",
       " '▁Instead']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(torch.squeeze(top_k_ids).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "first_token_id, second_token_id, third_token_id, fourth_token_id =  3869,1939, 29900,  1670\n",
    "first_token_list = []\n",
    "second_token_list = []\n",
    "third_token_list = []\n",
    "fourth_token_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    mapping_layer = model.get_output_embeddings()\n",
    "    for i in range(1, 33):\n",
    "        if i == 32:\n",
    "            out = mapping_layer(output_dict.hidden_states[0][i][:,-1,:])\n",
    "        else:\n",
    "            out = mapping_layer(model.model.norm(output_dict.hidden_states[0][i][:,-1,:]))\n",
    "        next_probs = F.softmax(out, dim=-1)\n",
    "        first_token_list.append(next_probs[:,first_token_id].cpu().numpy())\n",
    "        second_token_list.append(next_probs[:,second_token_id].cpu().numpy())        \n",
    "        third_token_list.append(next_probs[:,third_token_id].cpu().numpy())\n",
    "        fourth_token_list.append(next_probs[:,fourth_token_id].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#折线图\n",
    "x = np.arange(1, 33, 1)#点的横坐标\n",
    "k1 = first_token_list #线1的纵坐标\n",
    "k2 = second_token_list#线2的纵坐标\n",
    "k3 = third_token_list\n",
    "k4 = fourth_token_list\n",
    "plt.plot(x,k1,'s-',color = 'r',label=\"yes\")#s-:方形\n",
    "plt.plot(x,k2,'o-',color = 'g',label=\"no\")#o-:圆形 \n",
    "plt.plot(x,k3,'b',color = 'b',label=\"cup\")\n",
    "plt.plot(x,k4,'s-',color = 'b',label=\"chair\")\n",
    "plt.xlabel(\"layer\")#横坐标名字\n",
    "plt.ylabel(\"prob\")#纵坐标名字\n",
    "plt.legend(loc = \"best\")#图例\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dola_mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
