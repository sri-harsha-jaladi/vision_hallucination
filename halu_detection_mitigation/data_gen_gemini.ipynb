{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0c5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from google import genai\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.genai import types\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "SERVICE_ACCOUNT_FILE = \"/Data2/Arun-UAV/NLP/self-halu-detection/vertix_ai.json\"\n",
    "PROJECT_ID = \"hazel-math-472314-h9\"\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "COCO_JSON = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_sample_5000.json\"\n",
    "IMAGE_DIR = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\"\n",
    "OUTPUT_JSON = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_img_descriptions_parallel.json\"\n",
    "\n",
    "MAX_WORKERS = 64  # Adjust based on CPU count\n",
    "# ============================================\n",
    "\n",
    "# ---------- GEMINI CLIENT SETUP -------------\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    credentials=credentials,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772e0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/haloc/train_dataset/instruct/tp_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4da19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20k = data.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8908309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(question, ans_1, ans_2):\n",
    "  prompt = f\"\"\"\n",
    "You are given an **image** and **two question–answer (QA) pairs** related to that image.\n",
    "Your job is to analyze each QA pair separately and return **two lists of single words (no phrases)**:\n",
    "\n",
    "1. **`hallucination_candidates`** — all meaningful words that are semantically important for hallucination detection, belonging to categories like:\n",
    "\n",
    "   * **objects** (car, tree, person, lamp),\n",
    "   * **attributes** (red, shiny, tall),\n",
    "   * **relations** (on, under, behind, next_to),\n",
    "   * **actions** (running, holding, sitting),\n",
    "   * **count** (two, many, three),\n",
    "   * **scene/context** (beach, kitchen, street).\n",
    "   * **Decision tokens** yes, no, true, false, present, absent, exist, not, visible, correct\n",
    "\n",
    "   Include only **specific, content-bearing words** that could be visually verifiable or falsifiable from the image.\n",
    "   Do **not** include stopwords, determiners, or abstract terms.\n",
    "\n",
    "2. **`hallucinated_words`** — a subset of the above list that are **not visually supported** or are **contradicted** by the image content (i.e., hallucinated terms).\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "<attached image>\n",
    "\n",
    "qa_pair_1:\n",
    "Question: {question}\n",
    "Answer: {ans_1}\n",
    "\n",
    "qa_pair_2:\n",
    "Question: {question}\n",
    "Answer: {ans_2}\n",
    "\n",
    "\n",
    "**Output format (strict JSON per QA pair):**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"qa_pair_1\": {{\n",
    "    \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "    \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "  }},\n",
    "  \"qa_pair_2\": {{\n",
    "    \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "    \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "Note: extract \"hallucination_candidates\" and \"hallucinated_words\" from the Answer strictly not from the Question\n",
    "\n",
    "Ensure both lists contain **only lowercase single words** and are **deduplicated**.\n",
    "Focus on **important, visually grounded words** — not all tokens.\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c9fa629",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inx, row in data_20k.iloc[1:].iterrows():\n",
    "    question = row[\"prompt\"].replace(\"<image>\", \"\")\n",
    "    nh_ans  = row[\"source_text\"]\n",
    "    h_ans = row[\"hallucinated_text\"]\n",
    "    PROMPT = get_prompt(question, nh_ans, h_ans)\n",
    "    \n",
    "    img_path = f\"/Data2/Arun-UAV/NLP/vision_halu/visual_genome/VG_100K/{row['image_id']}.jpg\" \n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(\"Image not found\")\n",
    "\n",
    "    _, encoded_img = cv2.imencode(\".jpg\", image)\n",
    "    img_bytes = io.BytesIO(encoded_img.tobytes()).getvalue()\n",
    "\n",
    "    contents = [PROMPT, types.Part.from_bytes(data=img_bytes, mime_type=\"image/jpeg\")]\n",
    "\n",
    "    structured_config = types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        top_p=1.0,\n",
    "        top_k=32,\n",
    "        candidate_count=1,\n",
    "        max_output_tokens=65535,\n",
    "        response_mime_type=\"application/json\",\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=contents,\n",
    "        config=structured_config,\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d191fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b699c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33185b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
