{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552c53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from google import genai\n",
    "\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "SERVICE_ACCOUNT_FILE = \"/Data2/Arun-UAV/NLP/new_cloud_coount.json\"\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)\n",
    "\n",
    "client = storage.Client(credentials=credentials)\n",
    "\n",
    "gen_client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project='third-apex-476512-a7',   # or set directly\n",
    "    location='us-central1',    # or set directly, e.g. \"us-central1\"\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14b6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/evidence_head_train_datasets/finecops_ref/train_processed_finecopes_ref_30k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a465e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>objects_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2373626</td>\n",
       "      <td>The dish, positioned to the right of the gray ...</td>\n",
       "      <td>[{'object_id': '3725798', 'names': ['dish'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285719</td>\n",
       "      <td>On the left side of the metal and black fence,...</td>\n",
       "      <td>[{'object_id': '4401457', 'names': ['bike'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2383125</td>\n",
       "      <td>The post that is to the left of the car that i...</td>\n",
       "      <td>[{'object_id': '534946', 'names': ['car'], 'h'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2349520</td>\n",
       "      <td>The clock that is to the left of the tower tha...</td>\n",
       "      <td>[{'object_id': '3903072', 'names': ['roof'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2352548</td>\n",
       "      <td>The white keyboard positioned to the right of ...</td>\n",
       "      <td>[{'object_id': '1807596', 'names': ['computer'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                             answer  \\\n",
       "0   2373626  The dish, positioned to the right of the gray ...   \n",
       "1    285719  On the left side of the metal and black fence,...   \n",
       "2   2383125  The post that is to the left of the car that i...   \n",
       "3   2349520  The clock that is to the left of the tower tha...   \n",
       "4   2352548  The white keyboard positioned to the right of ...   \n",
       "\n",
       "                                        objects_info  \n",
       "0  [{'object_id': '3725798', 'names': ['dish'], '...  \n",
       "1  [{'object_id': '4401457', 'names': ['bike'], '...  \n",
       "2  [{'object_id': '534946', 'names': ['car'], 'h'...  \n",
       "3  [{'object_id': '3903072', 'names': ['roof'], '...  \n",
       "4  [{'object_id': '1807596', 'names': ['computer'...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d4a0cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>objects_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2373626</td>\n",
       "      <td>The dish, positioned to the right of the gray ...</td>\n",
       "      <td>[{'object_id': '3725798', 'names': ['dish'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285719</td>\n",
       "      <td>On the left side of the metal and black fence,...</td>\n",
       "      <td>[{'object_id': '4401457', 'names': ['bike'], '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                             answer  \\\n",
       "0   2373626  The dish, positioned to the right of the gray ...   \n",
       "1    285719  On the left side of the metal and black fence,...   \n",
       "\n",
       "                                        objects_info  \n",
       "0  [{'object_id': '3725798', 'names': ['dish'], '...  \n",
       "1  [{'object_id': '4401457', 'names': ['bike'], '...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cff6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop_duplicates(subset=[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95694844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29839"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e1cf0",
   "metadata": {},
   "source": [
    "# batch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c8b909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29839it [00:23, 1252.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_prompt(caption, labels):\n",
    "  prompt = f\"\"\"\n",
    "You are an expert visual reasoning model analyzing captions and scene descriptions.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Task 1 ‚Äî Question Generation**\n",
    "\n",
    "Given a **caption** that describes an image, generate **one natural, human-like question** that someone might ask about that image based on the caption.\n",
    "The question must:\n",
    "\n",
    "* Be **relevant**, **contextual**, and **grammatically natural**.\n",
    "* Avoid generic phrasing (‚ÄúWhat is shown here?‚Äù).\n",
    "* Focus on salient visual content, relationships, or actions implied in the caption.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Task 2 ‚Äî Semantic Word Extraction**\n",
    "\n",
    "You are also given a list of **objects with their IDs** that exist in the image.\n",
    "\n",
    "Analyze the same caption and extract all **meaningful, semantically important words** that can be linked to the given object IDs.\n",
    "Each extracted word must be categorized into one of the following classes:\n",
    "\n",
    "* **objects** ‚Äî tangible entities (e.g., car, tree, bowl)\n",
    "* **attributes** ‚Äî visual or descriptive traits (e.g., red, shiny, tall)\n",
    "* **relations** ‚Äî spatial or functional links (e.g., on, behind, holding)\n",
    "* **actions** ‚Äî verbs implying activity (e.g., running, sitting)\n",
    "* **count** ‚Äî numerical or quantity words (e.g., two, many)\n",
    "* **scene/context** ‚Äî setting or environment (e.g., street, kitchen)\n",
    "* **decision tokens** ‚Äî logical or binary cues (yes, no, visible, present, absent)\n",
    "\n",
    "Only include words that:\n",
    "\n",
    "* **Appear in or are implied by the caption**, and\n",
    "* **Can be associated with at least one given object ID.**\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ **Expected JSON Output Format**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"question\": \"What is the man holding on the table?\",\n",
    "  \"attributes\": [\n",
    "    {{\"word\": \"gray\", \"objects_involved\": [\"2428018\"]}},\n",
    "    {{\"word\": \"red\", \"objects_involved\": [\"2235351\"]}}\n",
    "  ],\n",
    "  \"relations\": [\n",
    "    {{\"word\": \"on\", \"objects_involved\": [\"2428018\", \"3725798\"]}}\n",
    "  ],\n",
    "  \"actions\": [],\n",
    "  \"count\": [],\n",
    "  \"decision_tokens\": []\n",
    "}}\n",
    "```\n",
    "\n",
    "Inputs:\n",
    "Caption: {caption}\n",
    "Objects with IDs: {labels}\n",
    "\"\"\"\n",
    "  return prompt\n",
    "\n",
    "all_res = []\n",
    "i = 0\n",
    "for inx, row in tqdm(final_df.iterrows()):\n",
    "    answer  = row[\"answer\"]\n",
    "    labels = pd.DataFrame(eval(row[\"objects_info\"]))[[\"names\", \"object_id\"]].to_dict(orient=\"records\")\n",
    "    PROMPT = get_prompt(answer, labels=labels)\n",
    "    res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}]}], \n",
    "                      \"generationConfig\": {\"temperature\": 0.8, \"topP\": 1, \"maxOutputTokens\": 2000,\"thinking_config\":{\"thinking_budget\":1000}}}}\n",
    "    all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3b8adc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29839"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e0a8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res)\n",
    "df.to_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29222894",
   "metadata": {},
   "source": [
    "# Uploading files to gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d038d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(local_path: str, gcs_uri: str):\n",
    "    \"\"\"\n",
    "    Upload a local file to a target GCS URI.\n",
    "\n",
    "    Args:\n",
    "        local_path (str): Local file path to upload.\n",
    "        gcs_uri (str): Target GCS URI like 'gs://my-bucket/path/to/upload.txt'\n",
    "        service_account_path (str): Path to GCP service account JSON.\n",
    "    \"\"\"\n",
    "    if not gcs_uri.startswith(\"gs://\"):\n",
    "        raise ValueError(\"Invalid GCS URI. Must start with gs://\")\n",
    "\n",
    "    parts = gcs_uri[5:].split(\"/\", 1)\n",
    "    bucket_name = parts[0]\n",
    "    blob_name = parts[1]\n",
    "\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(local_path)\n",
    "\n",
    "    print(f\"‚úÖ Uploaded {local_path} ‚Üí {gcs_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cf3a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded /Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl ‚Üí gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\n"
     ]
    }
   ],
   "source": [
    "upload_to_gcs(local_path=\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl\", gcs_uri = \"gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf87eca",
   "metadata": {},
   "source": [
    "# start batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9efc5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: projects/358874265041/locations/us-central1/batchPredictionJobs/4931218339570647040\n",
      "Job state: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import CreateBatchJobConfig, JobState, HttpOptions\n",
    "output_uri = \"gs://train_data_vision_1/gemini_batch_info/\"\n",
    "\n",
    "# See the documentation: https://googleapis.github.io/python-genai/genai.html#genai.batches.Batches.create\n",
    "job = gen_client.batches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    src=\"gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\",\n",
    "    config=CreateBatchJobConfig(dest=output_uri),\n",
    ")\n",
    "print(f\"Job name: {job.name}\")\n",
    "print(f\"Job state: {job.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9126c060",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m job_info \u001b[38;5;241m=\u001b[39m \u001b[43mgen_client\u001b[49m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m=\u001b[39mjob\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m      2\u001b[0m job_info\u001b[38;5;241m.\u001b[39mstate\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_client' is not defined"
     ]
    }
   ],
   "source": [
    "job_info = gen_client.batches.get(name=job.name)\n",
    "job_info.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf8435",
   "metadata": {},
   "source": [
    "# download batch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16b2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gcs(gcs_uri: str, local_path: str):\n",
    "    \"\"\"\n",
    "    Download a file from GCS based on its gs:// URI.\n",
    "\n",
    "    Args:\n",
    "        gcs_uri (str): GCS URI like 'gs://my-bucket/path/to/file.txt'\n",
    "        local_path (str): Path to store the downloaded file locally.\n",
    "    \"\"\"\n",
    "    # Parse bucket and blob name\n",
    "    if not gcs_uri.startswith(\"gs://\"):\n",
    "        raise ValueError(\"Invalid GCS URI. Must start with gs://\")\n",
    "\n",
    "    parts = gcs_uri[5:].split(\"/\", 1)\n",
    "    bucket_name = parts[0]\n",
    "    blob_name = parts[1]\n",
    "    \n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    blob.download_to_filename(local_path)\n",
    "\n",
    "    print(f\"‚úÖ Downloaded {gcs_uri} ‚Üí {local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7622509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded gs://train_data_vision_1/gemini_batch_info/prediction-model-2025-10-28T18:37:13.007798Z/predictions.jsonl ‚Üí /Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\n"
     ]
    }
   ],
   "source": [
    "download_from_gcs(gcs_uri=\"gs://train_data_vision_1/gemini_batch_info/prediction-model-2025-10-28T18:37:13.007798Z/predictions.jsonl\", local_path =\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24bb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.read_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f77d153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>status</th>\n",
       "      <th>response</th>\n",
       "      <th>processed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contents': [{'parts': [{'text': '\\nYou are a...</td>\n",
       "      <td></td>\n",
       "      <td>{'candidates': [{'avgLogprobs': -1.20831563849...</td>\n",
       "      <td>2025-10-28 18:43:47.966000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'contents': [{'parts': [{'text': '\\nYou are a...</td>\n",
       "      <td></td>\n",
       "      <td>{'candidates': [{'avgLogprobs': -1.15532089992...</td>\n",
       "      <td>2025-10-28 18:43:47.980000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'contents': [{'parts': [{'text': '\\nYou are a...</td>\n",
       "      <td></td>\n",
       "      <td>{'candidates': [{'avgLogprobs': -0.88970524249...</td>\n",
       "      <td>2025-10-28 18:43:47.973000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'contents': [{'parts': [{'text': '\\nYou are a...</td>\n",
       "      <td></td>\n",
       "      <td>{'candidates': [{'avgLogprobs': -0.90098913092...</td>\n",
       "      <td>2025-10-28 18:43:47.983000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'contents': [{'parts': [{'text': '\\nYou are a...</td>\n",
       "      <td></td>\n",
       "      <td>{'candidates': [{'avgLogprobs': -0.67929188992...</td>\n",
       "      <td>2025-10-28 18:43:47.985000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             request status  \\\n",
       "0  {'contents': [{'parts': [{'text': '\\nYou are a...          \n",
       "1  {'contents': [{'parts': [{'text': '\\nYou are a...          \n",
       "2  {'contents': [{'parts': [{'text': '\\nYou are a...          \n",
       "3  {'contents': [{'parts': [{'text': '\\nYou are a...          \n",
       "4  {'contents': [{'parts': [{'text': '\\nYou are a...          \n",
       "\n",
       "                                            response  \\\n",
       "0  {'candidates': [{'avgLogprobs': -1.20831563849...   \n",
       "1  {'candidates': [{'avgLogprobs': -1.15532089992...   \n",
       "2  {'candidates': [{'avgLogprobs': -0.88970524249...   \n",
       "3  {'candidates': [{'avgLogprobs': -0.90098913092...   \n",
       "4  {'candidates': [{'avgLogprobs': -0.67929188992...   \n",
       "\n",
       "                    processed_time  \n",
       "0 2025-10-28 18:43:47.966000+00:00  \n",
       "1 2025-10-28 18:43:47.980000+00:00  \n",
       "2 2025-10-28 18:43:47.973000+00:00  \n",
       "3 2025-10-28 18:43:47.983000+00:00  \n",
       "4 2025-10-28 18:43:47.985000+00:00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c34b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = eval(row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].replace(\"json\", \"\").strip(\"```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ebee9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Where is the woman located in relation to the man?',\n",
       " 'attributes': [{'word': 'walking', 'objects_involved': ['150916']}],\n",
       " 'relations': [{'word': 'left of', 'objects_involved': ['150910', '150924']},\n",
       "  {'word': 'left of', 'objects_involved': ['150924', '150916']}],\n",
       " 'actions': [],\n",
       " 'count': [],\n",
       " 'decision_tokens': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650d2f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Where is the woman located in relation to the man?',\n",
       " 'attributes': [{'word': 'walking', 'objects_involved': ['150916']}],\n",
       " 'relations': [{'word': 'left of', 'objects_involved': ['150910', '150924']},\n",
       "  {'word': 'left of', 'objects_involved': ['150924', '150916']}],\n",
       " 'actions': [],\n",
       " 'count': [],\n",
       " 'decision_tokens': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7ad7630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where is the woman located in relation to the man?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1875c59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Situated to the left of the building, which is to the left of the walking man, is the woman.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2028b49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>names</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150924</td>\n",
       "      <td>[building]</td>\n",
       "      <td>343</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150916</td>\n",
       "      <td>[man]</td>\n",
       "      <td>91</td>\n",
       "      <td>44</td>\n",
       "      <td>246</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150910</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>208</td>\n",
       "      <td>88</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  object_id       names    h   w    y    x\n",
       "0    150924  [building]  343  80    7   57\n",
       "1    150916       [man]   91  44  246  220\n",
       "2    150910     [woman]  208  88  258    0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37e16143",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_df = pd.DataFrame(res[\"attributes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8ae5151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>names</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150924</td>\n",
       "      <td>[building]</td>\n",
       "      <td>343</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150916</td>\n",
       "      <td>[man]</td>\n",
       "      <td>91</td>\n",
       "      <td>44</td>\n",
       "      <td>246</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150910</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>208</td>\n",
       "      <td>88</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  object_id       names    h   w    y    x\n",
       "0    150924  [building]  343  80    7   57\n",
       "1    150916       [man]   91  44  246  220\n",
       "2    150910     [woman]  208  88  258    0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59b54c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>objects_involved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walking</td>\n",
       "      <td>[150916]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word objects_involved\n",
       "0  walking         [150916]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dafa7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_2_bbox(target_df, objects_df):\n",
    "    all_bbox_map = []\n",
    "    for row in target_df.iterrows():\n",
    "        word = row[1][\"word\"]\n",
    "        obj_ids = row[1][\"objects_involved\"]\n",
    "        bbox = objects_df[objects_df[\"object_id\"].isin(obj_ids)][[\"h\", \"w\", \"y\", \"x\"]].to_dict(orient=\"records\")[0]\n",
    "        bbox_map = {}\n",
    "        bbox_map[\"word\"] = word\n",
    "        bbox_map.update(bbox)\n",
    "        all_bbox_map.append(bbox_map)\n",
    "    return all_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "091f9e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'word': 'walking', 'h': 91, 'w': 44, 'y': 246, 'x': 220}],\n",
       " [{'word': 'left of', 'h': 343, 'w': 80, 'y': 7, 'x': 57},\n",
       "  {'word': 'left of', 'h': 343, 'w': 80, 'y': 7, 'x': 57}]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "562467cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'building', 'h': 343, 'w': 80, 'y': 7, 'x': 57},\n",
       " {'word': 'man', 'h': 91, 'w': 44, 'y': 246, 'x': 220},\n",
       " {'word': 'woman', 'h': 208, 'w': 88, 'y': 258, 'x': 0},\n",
       " {'word': 'walking', 'h': 91, 'w': 44, 'y': 246, 'x': 220},\n",
       " {'word': 'left of', 'h': 343, 'w': 80, 'y': 7, 'x': 57},\n",
       " {'word': 'left of', 'h': 343, 'w': 80, 'y': 7, 'x': 57}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6424366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29839it [03:59, 124.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_res = []\n",
    "error = 0\n",
    "for inx, row in tqdm(pred_data.iterrows()):\n",
    "\n",
    "    try:\n",
    "        res = eval(row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].replace(\"json\", \"\").strip(\"```\"))\n",
    "        question = res[\"question\"]\n",
    "        caption = row[\"request\"][\"contents\"][0][\"parts\"][0][\"text\"].split(\"Caption:\")[-1].split(\"Objects with IDs:\")[0].strip()\n",
    "        req_info = final_df[final_df[\"answer\"] == caption]\n",
    "        img_id = str(int(req_info[\"image_id\"].values[0]))\n",
    "        objects_df =  pd.DataFrame(eval(req_info[\"objects_info\"].values[0]))\n",
    "        objects_df[\"names\"] = objects_df[\"names\"].apply(lambda x: x[0])\n",
    "        req_obj_df = objects_df[[\"names\", \"h\", \"w\", \"y\", \"x\"]]\n",
    "        req_obj_df.columns = [\"word\", \"h\", \"w\", \"y\", \"x\"]\n",
    "        \n",
    "        target_words = req_obj_df.to_dict(orient=\"records\")\n",
    "        try:\n",
    "            attr_df = pd.DataFrame(res[\"attributes\"])\n",
    "            if attr_df.shape[0]>0:\n",
    "                attr_df = attr_df[attr_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                att_words = get_word_2_bbox(attr_df, objects_df)\n",
    "                target_words.extend(att_words)\n",
    "\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "            \n",
    "        try:\n",
    "            rel_df = pd.DataFrame(res[\"relations\"])\n",
    "            if rel_df.shape[0]>0:\n",
    "                rel_df = rel_df[rel_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                rel_words = get_word_2_bbox(rel_df, objects_df)\n",
    "                target_words.extend(rel_words)\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "        \n",
    "        try:\n",
    "            count_df = pd.DataFrame(res[\"count\"])\n",
    "            if count_df.shape[0]>0:\n",
    "                count_df = count_df[count_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                count_words = get_word_2_bbox(count_df, objects_df)\n",
    "                target_words.extend(count_words)\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "\n",
    "        try:\n",
    "            decision_df = pd.DataFrame(res[\"decision_tokens\"])\n",
    "            if decision_df.shape[0]>0:\n",
    "                decision_df = decision_df[decision_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                decision_words = get_word_2_bbox(decision_df, objects_df)\n",
    "            target_words.extend(decision_words)\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "        \n",
    "        all_res.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"question\": question,\n",
    "            \"caption\": caption,\n",
    "            \"target_words\": target_words\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        error += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8941c393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29831"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a1318849",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_res_df = pd.DataFrame(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81980062",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_res_df = total_res_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ee7326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_res_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/evidence_head_train_datasets/finecops_ref/final_finecopes_ref_30k_bb_annot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5971bca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31713"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce5e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667351af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'          image_id        filename phrase object_name  xmin  ymin  xmax  ymax  \\\\\\n194344  1191423753  1191423753.jpg   4934        4934   351   177   405   298   \\n194345  1191423753  1191423753.jpg   4934        4934    63   223   103   349   \\n194346  1191423753  1191423753.jpg   4934        4934    11   215    78   360   \\n194347  1191423753  1191423753.jpg   4935        4935     1     1   346   344   \\n194348  1191423753  1191423753.jpg   4936        4936   268   165   317   276   \\n194349  1191423753  1191423753.jpg   4937        4937   222   179   263   307   \\n194350  1191423753  1191423753.jpg   4937        4937   117   179   178   320   \\n194351  1191423753  1191423753.jpg   4937        4937   161   187   198   309   \\n194352  1191423753  1191423753.jpg   4937        4937   192   177   228   313   \\n194353  1191423753  1191423753.jpg   4937        4937   264   154   322   284   \\n194354  1191423753  1191423753.jpg   4938        4938   312   196   348   249   \\n194355  1191423753  1191423753.jpg   4938        4938   264   281   330   307   \\n194356  1191423753  1191423753.jpg   4940        4940   124   204   169   253   \\n194357  1191423753  1191423753.jpg   4940        4940   160   206   201   244   \\n194358  1191423753  1191423753.jpg   4940        4940   193   193   230   243   \\n194359  1191423753  1191423753.jpg   4940        4940   220   195   261   235   \\n194360  1191423753  1191423753.jpg   4941        4941   132   238   168   308   \\n194361  1191423753  1191423753.jpg   4941        4941    64   275    98   342   \\n194362  1191423753  1191423753.jpg   4941        4941   227   231   253   298   \\n194363  1191423753  1191423753.jpg   4941        4941   199   235   231   306   \\n194364  1191423753  1191423753.jpg   4941        4941   165   237   196   299   \\n194365  1191423753  1191423753.jpg   4943        4943    60   182   108   319   \\n194366  1191423753  1191423753.jpg   4944        4944     4   315   495   375   \\n194367  1191423753  1191423753.jpg   4944        4944   346   210   500   310   \\n\\n        width  height                                          sentences  \\n194344    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194345    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194346    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194347    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194348    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194349    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194350    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194351    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194352    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194353    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194354    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194355    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194356    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194357    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194358    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194359    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194360    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194361    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194362    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194363    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194364    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194365    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194366    500     375  [[/EN#4934/people Two people] standing in fron...  \\n194367    500     375  [[/EN#4934/people Two people] standing in fron...  '"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.iloc[0][\"bb_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a0d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/evidence_head_train_datasets/flicker/train_processed_flickr_30k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab3006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>caption</th>\n",
       "      <th>question</th>\n",
       "      <th>expanded_labels</th>\n",
       "      <th>image_id</th>\n",
       "      <th>bb_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Three people are on a sidewalk.</td>\n",
       "      <td>How many people are on the sidewalk?</td>\n",
       "      <td>{'Three': '4934', 'people': '4934', 'sidewalk'...</td>\n",
       "      <td>1191423753</td>\n",
       "      <td>image_id        filename phrase obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beautiful brunette woman, draped in purple, bl...</td>\n",
       "      <td>What colors are the scarves draped around the ...</td>\n",
       "      <td>{'Beautiful': '9911', 'brunette': '9911', 'wom...</td>\n",
       "      <td>1360456780</td>\n",
       "      <td>image_id        filename phrase objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A woman is welding metal at a work table.</td>\n",
       "      <td>What is the woman doing at the work table?</td>\n",
       "      <td>{'woman': '22853', 'metal': '22855', 'work': '...</td>\n",
       "      <td>1897067588</td>\n",
       "      <td>image_id        filename phrase obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A girl in a white shirt is holding a ball and ...</td>\n",
       "      <td>What is the girl pointing at?</td>\n",
       "      <td>{'girl': '27660', 'white': '27663', 'shirt': '...</td>\n",
       "      <td>208472767</td>\n",
       "      <td>image_id       filename phrase object...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The large, dark colored dog is doing a climbin...</td>\n",
       "      <td>What is the large, dark-colored dog doing?</td>\n",
       "      <td>{'large': '32430', 'dark': '32430', 'colored':...</td>\n",
       "      <td>2176364472</td>\n",
       "      <td>image_id        filename phrase obje...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            caption  \\\n",
       "0           0                    Three people are on a sidewalk.   \n",
       "1           1  Beautiful brunette woman, draped in purple, bl...   \n",
       "2           2          A woman is welding metal at a work table.   \n",
       "3           3  A girl in a white shirt is holding a ball and ...   \n",
       "4           4  The large, dark colored dog is doing a climbin...   \n",
       "\n",
       "                                            question  \\\n",
       "0               How many people are on the sidewalk?   \n",
       "1  What colors are the scarves draped around the ...   \n",
       "2         What is the woman doing at the work table?   \n",
       "3                      What is the girl pointing at?   \n",
       "4         What is the large, dark-colored dog doing?   \n",
       "\n",
       "                                     expanded_labels    image_id  \\\n",
       "0  {'Three': '4934', 'people': '4934', 'sidewalk'...  1191423753   \n",
       "1  {'Beautiful': '9911', 'brunette': '9911', 'wom...  1360456780   \n",
       "2  {'woman': '22853', 'metal': '22855', 'work': '...  1897067588   \n",
       "3  {'girl': '27660', 'white': '27663', 'shirt': '...   208472767   \n",
       "4  {'large': '32430', 'dark': '32430', 'colored':...  2176364472   \n",
       "\n",
       "                                               bb_df  \n",
       "0            image_id        filename phrase obje...  \n",
       "1           image_id        filename phrase objec...  \n",
       "2            image_id        filename phrase obje...  \n",
       "3           image_id       filename phrase object...  \n",
       "4            image_id        filename phrase obje...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ed6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a94496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a person behind the street lamp?\n",
      "The street lamp is the focal point, but there is a person bent down directly behind the lamp.\n",
      "{'street': '218332', 'lamp': '218332', 'focal': '218334', 'point': '218334', 'person': '218336'}\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print(df[\"question\"].iloc[i])\n",
    "print(df[\"caption\"].iloc[i])\n",
    "print(df[\"expanded_labels\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac0288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'          image_id        filename  phrase object_name  xmin  ymin  xmax  \\\\\\n228801  4919450790  4919450790.jpg  218328      218328     2   208   500   \\n228802  4919450790  4919450790.jpg  218329      218329     5   191   497   \\n228803  4919450790  4919450790.jpg  218330      218330   299    29   500   \\n228804  4919450790  4919450790.jpg  218332      218332   260    79   288   \\n228805  4919450790  4919450790.jpg  218335      218335   262    81   282   \\n228806  4919450790  4919450790.jpg  218336      218336   217   182   240   \\n\\n        ymax  width  height                                          sentences  \\n228801   229    500     271  [[/EN#218326/scene A city] with [/EN#218330/sc...  \\n228802   258    500     271  [[/EN#218326/scene A city] with [/EN#218330/sc...  \\n228803   225    500     271  [[/EN#218326/scene A city] with [/EN#218330/sc...  \\n228804   193    500     271  [[/EN#218326/scene A city] with [/EN#218330/sc...  \\n228805   225    500     271  [[/EN#218326/scene A city] with [/EN#218330/sc...  \\n228806   220    500     271  [[/EN#218326/scene A city] with [/EN#218330/sc...  '"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"bb_df\"].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dedc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_str = df[\"bb_df\"].iloc[i]\n",
    "nested_df = pd.read_fwf(StringIO(nested_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db25ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934897e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
