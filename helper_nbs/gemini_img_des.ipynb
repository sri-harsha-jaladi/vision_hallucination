{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d54e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from google import genai\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.genai import types\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f462cd19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROMPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m img_file_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     85\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m img_file_name\n\u001b[0;32m---> 86\u001b[0m img_des \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_des\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m all_img_des\u001b[38;5;241m.\u001b[39mappend(img_des)\n",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m, in \u001b[0;36mget_image_des\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m _, total_encoded_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, image)\n\u001b[1;32m     54\u001b[0m total_img_bytes \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(total_encoded_img\u001b[38;5;241m.\u001b[39mtobytes())\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m---> 56\u001b[0m image_contents \u001b[38;5;241m=\u001b[39m [\u001b[43mPROMPT\u001b[49m, types\u001b[38;5;241m.\u001b[39mPart\u001b[38;5;241m.\u001b[39mfrom_bytes(data\u001b[38;5;241m=\u001b[39mtotal_img_bytes, mime_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     58\u001b[0m structured_generation_config \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentConfig(\n\u001b[1;32m     59\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m     60\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     thinking_config\u001b[38;5;241m=\u001b[39mtypes\u001b[38;5;241m.\u001b[39m ThinkingConfig(thinking_budget\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m     70\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m     contents\u001b[38;5;241m=\u001b[39mimage_contents,\n\u001b[1;32m     72\u001b[0m     config\u001b[38;5;241m=\u001b[39mstructured_generation_config,\n\u001b[1;32m     73\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PROMPT' is not defined"
     ]
    }
   ],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = \"/Data2/Arun-UAV/NLP/self-halu-detection/vertix_ai.json\"\n",
    "\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project='hazel-math-472314-h9',   # or set directly\n",
    "    location='us-central1',    # or set directly, e.g. \"us-central1\"\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    image_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of the given image.\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are specialist in rich and precise scene understanding.\n",
    "Given an input image, generate a comprehensive, contextually aware, and fluent description that captures all key visual elements, their relationships, emotions, and possible context or story.\n",
    "\n",
    "Your description should go beyond short captions — it must resemble a paragraph of visual storytelling that includes:\n",
    "\n",
    "Scene type: indoor/outdoor, environment, lighting, time of day\n",
    "\n",
    "Objects and entities: names, counts, shapes, colors, materials\n",
    "\n",
    "Actions and interactions: what the people or objects are doing\n",
    "\n",
    "Spatial layout: foreground, background, relative positions\n",
    "\n",
    "Emotions or atmosphere: tone, mood, aesthetics\n",
    "\n",
    "Possible context: what might be happening or implied by the scene\n",
    "\n",
    "Avoid generic or repetitive statements. Be vivid, factual, and coherent. Use natural language instead of bullet points.\n",
    "\n",
    "Output json Format:\n",
    "\n",
    "{image_description: <full attached image description>}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_image_des(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    _, total_encoded_img = cv2.imencode(\".jpg\", image)\n",
    "    total_img_bytes = io.BytesIO(total_encoded_img.tobytes()).getvalue()\n",
    "\n",
    "    image_contents = [PROMPT, types.Part.from_bytes(data=total_img_bytes, mime_type=\"image/jpeg\")]\n",
    "\n",
    "    structured_generation_config = types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        top_p=1.0,\n",
    "        top_k=32,\n",
    "        candidate_count=1,\n",
    "        max_output_tokens=65535,\n",
    "        response_schema=ImageDescription,\n",
    "        response_mime_type=\"application/json\",\n",
    "        thinking_config=types. ThinkingConfig(thinking_budget=0),\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=image_contents,\n",
    "        config=structured_generation_config,\n",
    "    )\n",
    "    \n",
    "    return response.parsed.model_dump()\n",
    "\n",
    "\n",
    "coco_annot_data = pd.read_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_sample_5000.json\")\n",
    "img_list = os.listdir(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\")\n",
    "\n",
    "\n",
    "all_img_des = []\n",
    "for inx, row in coco_annot_data.iterrows():\n",
    "    img_file_name = row[\"file_name\"]\n",
    "    img_path = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\"+ \"/\" + img_file_name\n",
    "    img_des = get_image_des(img_path)\n",
    "    all_img_des.append(img_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_des = []\n",
    "for inx, row in coco_annot_data.iterrows():\n",
    "    img_file_name = row[\"file_name\"]\n",
    "    if img_file_name in img_list:\n",
    "        img_path = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\"+ \"/\" + img_file_name\n",
    "        img_des = get_image_des(img_path)\n",
    "        all_img_des.append(img_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1248170b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_file_name in img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f50889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a08bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = \"/Data2/Arun-UAV/NLP/self-halu-detection/vertix_ai.json\"\n",
    "\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project='hazel-math-472314-h9',   # or set directly\n",
    "    location='us-central1',    # or set directly, e.g. \"us-central1\"\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac75dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePatchDescription(BaseModel):\n",
    "    total_image_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of the entire image (global context).\"\n",
    "    )\n",
    "    top_left_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of the top-left patch.\"\n",
    "    )\n",
    "    top_right_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of the top-right patch.\"\n",
    "    )\n",
    "    bottom_left_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of the bottom-left patch.\"\n",
    "    )\n",
    "    bottom_right_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of the bottom-right patch.\"\n",
    "    )\n",
    "    \n",
    "class ImageDescription(BaseModel):\n",
    "    image_description: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of the given image.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a30c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are a multimodal language model that receives one **total image** and its four **quadrant patches** (non-overlapping, obtained by bisecting the image horizontally and vertically). Your task is to generate **detailed, human-readable descriptions** of:\n",
    "\n",
    "1. The **entire image** (holistic/global view).\n",
    "2. Each **patch individually**, while using the total image only for context.\n",
    "\n",
    "**Rules:**\n",
    "\n",
    "* **Global Description** → capture layout, objects, relationships, background etc.. like every detail\n",
    "* **Patch Descriptions** → focus **only on that patch**.\n",
    "\n",
    "  * Be precise: mention objects, colors, textures, shapes, activities, etc without missing any key details.\n",
    "  * Optionally relate to global context (e.g., “this is the left side of the building visible in the full image”).\n",
    "* Output must be **strictly valid JSON**.\n",
    "\n",
    "**Output Format (strict JSON):**\n",
    "\n",
    "```json\n",
    "{{\n",
    "   \"total_image_description\": \"<detailed description of the entire image>\",\n",
    "   \"top_left_description\": \"<detailed description of the top-left patch>\",\n",
    "   \"top_right_description\": \"<detailed description of the top-right patch>\",\n",
    "   \"bottom_left_description\": \"<detailed description of the bottom-left patch>\",\n",
    "   \"bottom_right_description\": \"<detailed description of the bottom-right patch>\"\n",
    "}}\n",
    "\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d91d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are specialist in rich and precise scene understanding.\n",
    "Given an input image, generate a comprehensive, contextually aware, and fluent description that captures all key visual elements, their relationships, emotions, and possible context or story.\n",
    "\n",
    "Your description should go beyond short captions — it must resemble a paragraph of visual storytelling that includes:\n",
    "\n",
    "Scene type: indoor/outdoor, environment, lighting, time of day\n",
    "\n",
    "Objects and entities: names, counts, shapes, colors, materials\n",
    "\n",
    "Actions and interactions: what the people or objects are doing\n",
    "\n",
    "Spatial layout: foreground, background, relative positions\n",
    "\n",
    "Emotions or atmosphere: tone, mood, aesthetics\n",
    "\n",
    "Possible context: what might be happening or implied by the scene\n",
    "\n",
    "Avoid generic or repetitive statements. Be vivid, factual, and coherent. Use natural language instead of bullet points.\n",
    "\n",
    "Output json Format:\n",
    "\n",
    "{image_description: <full attached image description>}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51680bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_des_patches(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, c = image.shape\n",
    "\n",
    "    # Compute mid points\n",
    "    mid_h, mid_w = h // 2, w // 2\n",
    "\n",
    "    # Split into 4 sub-images\n",
    "    top_left     = image[0:mid_h, 0:mid_w]\n",
    "    top_right    = image[0:mid_h, mid_w:w]\n",
    "    bottom_left  = image[mid_h:h, 0:mid_w]\n",
    "    bottom_right = image[mid_h:h, mid_w:w]\n",
    "\n",
    "    # Function to convert patch into JPEG bytes\n",
    "    def to_jpeg_bytes(patch):\n",
    "        success, encoded_img = cv2.imencode(\".jpg\", patch)\n",
    "        if not success:\n",
    "            raise ValueError(\"Encoding to JPEG failed!\")\n",
    "        img_bytes = io.BytesIO(encoded_img.tobytes()).getvalue()\n",
    "        return img_bytes\n",
    "\n",
    "    _, total_encoded_img = cv2.imencode(\".jpg\", image)\n",
    "    total_img_bytes = io.BytesIO(total_encoded_img.tobytes()).getvalue()\n",
    "\n",
    "    image_contents = [types.Part.from_bytes(data=total_img_bytes, mime_type=\"image/jpeg\")]\n",
    "    patch_bytes = {\n",
    "        \"top_left\": to_jpeg_bytes(top_left),\n",
    "        \"top_right\": to_jpeg_bytes(top_right),\n",
    "        \"bottom_left\": to_jpeg_bytes(bottom_left),\n",
    "        \"bottom_right\": to_jpeg_bytes(bottom_right)\n",
    "    }\n",
    "\n",
    "    for key,value in patch_bytes.items():\n",
    "        image_contents.append(types.Part.from_bytes(data=value, mime_type=\"image/jpeg\"))\n",
    "\n",
    "    image_contents.insert(0, PROMPT)\n",
    "\n",
    "    structured_generation_config = types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        top_p=1.0,\n",
    "        top_k=32,\n",
    "        candidate_count=1,\n",
    "        max_output_tokens=65535,\n",
    "        response_schema=ImagePatchDescription,\n",
    "        response_mime_type=\"application/json\",\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=image_contents,\n",
    "        config=structured_generation_config,\n",
    "    )\n",
    "    \n",
    "    return response.parsed.model_dump()\n",
    "\n",
    "\n",
    "def get_image_des(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    _, total_encoded_img = cv2.imencode(\".jpg\", image)\n",
    "    total_img_bytes = io.BytesIO(total_encoded_img.tobytes()).getvalue()\n",
    "\n",
    "    image_contents = [PROMPT, types.Part.from_bytes(data=total_img_bytes, mime_type=\"image/jpeg\")]\n",
    "\n",
    "    structured_generation_config = types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        top_p=1.0,\n",
    "        top_k=32,\n",
    "        candidate_count=1,\n",
    "        max_output_tokens=65535,\n",
    "        response_schema=ImageDescription,\n",
    "        response_mime_type=\"application/json\",\n",
    "        thinking_config=types. ThinkingConfig(thinking_budget=0),\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=image_contents,\n",
    "        config=structured_generation_config,\n",
    "    )\n",
    "    \n",
    "    return response.parsed.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "135ec9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 133135.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import json\n",
    "\n",
    "# all_des = []\n",
    "all_lines = []\n",
    "with open(\"/Data2/Arun-UAV/NLP/vision_halu/Deco/opera_log/llava-1.5/ours.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for _, data_line in tqdm(enumerate(f.readlines()), total=500):\n",
    "        line = json.loads(data_line)\n",
    "        all_lines.append(line) \n",
    "        # idx = line[\"image_id\"]\n",
    "        # image_file = \"/Data2/Arun-UAV/NLP/vision_halu/benchmarks/coco2024/val2014/COCO_val2014_\" + str(idx).zfill(12) + \".jpg\"\n",
    "        # all_des.append(get_image_des(image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8a6850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_data = pd.read_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_sample_5000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f64133e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>coco_url</th>\n",
       "      <th>captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73262</td>\n",
       "      <td>COCO_train2014_000000073262.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_t...</td>\n",
       "      <td>[A skateboarder performing a trick next to a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130534</td>\n",
       "      <td>COCO_train2014_000000130534.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_t...</td>\n",
       "      <td>[A woman standing in a room in front of a TV.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                        file_name  \\\n",
       "0     73262  COCO_train2014_000000073262.jpg   \n",
       "1    130534  COCO_train2014_000000130534.jpg   \n",
       "\n",
       "                                            coco_url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_t...   \n",
       "1  http://images.cocodataset.org/train2014/COCO_t...   \n",
       "\n",
       "                                            captions  \n",
       "0  [A skateboarder performing a trick next to a b...  \n",
       "1  [A woman standing in a room in front of a TV.,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f509b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = os.listdir(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\")\n",
    "img_path = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\"+ \"/\" + img_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82514f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "des = pd.read_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_img_descriptions_parallel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92c99f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HTTPSConnectionPool(host=\\'oauth2.googleapis.com\\', port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f5953bf7760>: Failed to resolve \\'oauth2.googleapis.com\\' ([Errno -3] Temporary failure in name resolution)\"))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des[\"error\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acc3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecb4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from google import genai\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.genai import types\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "SERVICE_ACCOUNT_FILE = \"/Data2/Arun-UAV/NLP/self-halu-detection/vertix_ai.json\"\n",
    "PROJECT_ID = \"hazel-math-472314-h9\"\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "COCO_JSON = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_sample_5000.json\"\n",
    "IMAGE_DIR = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\"\n",
    "OUTPUT_JSON = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_img_descriptions_parallel.json\"\n",
    "\n",
    "MAX_WORKERS = 64  # Adjust based on CPU count\n",
    "# ============================================\n",
    "\n",
    "# ---------- GEMINI CLIENT SETUP -------------\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    credentials=credentials,\n",
    ")\n",
    "# --------------------------------------------\n",
    "\n",
    "# ---------- PROMPT & RESPONSE SCHEMA --------\n",
    "class ImageDescription(BaseModel):\n",
    "    image_description: str = Field(\n",
    "        ..., description=\"Detailed description of the given image.\"\n",
    "    )\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are a specialist in rich and precise scene understanding.\n",
    "Given an input image, generate a comprehensive, contextually aware, and fluent description that captures all key visual elements, their relationships, emotions, and possible context or story.\n",
    "\n",
    "Your description should go beyond short captions — it must resemble a paragraph of visual storytelling that includes:\n",
    "\n",
    "Scene type: indoor/outdoor, environment, lighting, time of day\n",
    "Objects and entities: names, counts, shapes, colors, materials\n",
    "Actions and interactions: what the people or objects are doing\n",
    "Spatial layout: foreground, background, relative positions\n",
    "Emotions or atmosphere: tone, mood, aesthetics\n",
    "Possible context: what might be happening or implied by the scene\n",
    "\n",
    "Avoid generic or repetitive statements. Be vivid, factual, and coherent. Use natural language instead of bullet points.\n",
    "\n",
    "Output JSON format:\n",
    "{image_description: <full attached image description>}\n",
    "\"\"\"\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "def get_image_description(img_path: str):\n",
    "    \"\"\"Worker function: generate detailed image description.\"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            return {\"file_name\": os.path.basename(img_path), \"error\": \"Image not found\"}\n",
    "\n",
    "        _, encoded_img = cv2.imencode(\".jpg\", image)\n",
    "        img_bytes = io.BytesIO(encoded_img.tobytes()).getvalue()\n",
    "\n",
    "        contents = [PROMPT, types.Part.from_bytes(data=img_bytes, mime_type=\"image/jpeg\")]\n",
    "\n",
    "        structured_config = types.GenerateContentConfig(\n",
    "            temperature=0.6,\n",
    "            top_p=1.0,\n",
    "            top_k=32,\n",
    "            candidate_count=1,\n",
    "            max_output_tokens=65535,\n",
    "            response_schema=ImageDescription,\n",
    "            response_mime_type=\"application/json\",\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "        )\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=contents,\n",
    "            config=structured_config,\n",
    "        )\n",
    "\n",
    "        parsed = response.parsed.model_dump()\n",
    "        parsed[\"file_name\"] = os.path.basename(img_path)\n",
    "        return parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"file_name\": os.path.basename(img_path), \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7060849",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_data = pd.read_json(COCO_JSON)\n",
    "img_files = coco_data[\"file_name\"].tolist()\n",
    "img_list = os.listdir(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {\n",
    "        executor.submit(get_image_description, os.path.join(IMAGE_DIR, f)): f\n",
    "        for f in img_files if f in img_list\n",
    "    }\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing images\"):\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "\n",
    "# Save all results to JSON\n",
    "with open(OUTPUT_JSON, \"w\") as f_out:\n",
    "    json.dump(results, f_out, indent=2)\n",
    "\n",
    "print(f\"✅ Done! Saved {len(results)} image descriptions to {OUTPUT_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75862f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in img_files:\n",
    "    if f in img_list:\n",
    "        path = os.path.join(IMAGE_DIR, f)\n",
    "        des = get_image_description(path)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88ebcb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_description': 'The image captures an exhilarating outdoor scene at a skate park on a bright, sunny day with a vibrant blue sky dotted with fluffy white clouds. In the foreground, a young man in a red t-shirt, blue jeans, and a grey beanie is intensely focused while riding a bright neon green BMX bike on the edge of a concrete ramp. His body is slightly hunched over the handlebars, conveying speed and control. Directly above and slightly behind him, another young man, dressed in a yellow t-shirt, dark blue jeans, and a light blue baseball cap, is captured mid-air performing a trick on a skateboard. He is suspended high above the ramp, arms outstretched for balance, with his skateboard perfectly aligned beneath his feet. The dynamic composition creates a sense of simultaneous action and youthful energy. In the mid-ground, the concrete skate park features various ramps and structures, including a white wall with some graffiti and a concrete bench. Further back, several lush green trees line the perimeter of the park, and beyond them, residential houses and more greenery are visible under the clear sky, suggesting a suburban setting. To the far right, another person can be seen on a unicycle, adding to the lively atmosphere of extreme sports. The overall mood is energetic and spirited, highlighting the thrill of skateboarding and BMX riding under the open sky.',\n",
       " 'file_name': 'COCO_train2014_000000073262.jpg'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef9358a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_5000_gcp_upload_urs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee57ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT = \"\"\"\n",
    "You are a specialist in rich and precise scene understanding.\n",
    "Given an input image, generate a comprehensive, contextually aware, and fluent description that captures all key visual elements, their relationships, emotions, and possible context or story.\n",
    "\n",
    "Your description should go beyond short captions — it must resemble a paragraph of visual storytelling that includes:\n",
    "\n",
    "Scene type: indoor/outdoor, environment, lighting, time of day\n",
    "Objects and entities: names, counts, shapes, colors, materials\n",
    "Actions and interactions: what the people or objects are doing\n",
    "Spatial layout: foreground, background, relative positions\n",
    "Emotions or atmosphere: tone, mood, aesthetics\n",
    "Possible context: what might be happening or implied by the scene\n",
    "\n",
    "Avoid generic or repetitive statements. Be vivid, factual, and coherent. Use natural language instead of bullet points.\n",
    "\n",
    "Output JSON format:\n",
    "{image_description: <full attached image description>}\n",
    "\"\"\"\n",
    "\n",
    "all_res = []\n",
    "for uri in df[\"gcs_uri\"].tolist():\n",
    "    res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "                      \"generationConfig\": {\"temperature\": 0.6, \"topP\": 1, \"maxOutputTokens\": 1000,\"thinking_config\":{\"thinking_budget\":0}}}}\n",
    "    all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3483e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcf6574b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             request\n",
       "0  {'contents': [{'role': 'user', 'parts': [{'tex...\n",
       "1  {'contents': [{'role': 'user', 'parts': [{'tex..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dd17411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:10].to_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch_10_testing.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11d013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
