{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from google import genai\n",
    "\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "SERVICE_ACCOUNT_FILE = \"/Data2/Arun-UAV/NLP/new_cloud_coount.json\"\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)\n",
    "\n",
    "client = storage.Client(credentials=credentials)\n",
    "\n",
    "gen_client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project='third-apex-476512-a7',   # or set directly\n",
    "    location='us-central1',    # or set directly, e.g. \"us-central1\"\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e14b6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/evidence_head_train_datasets/finecops_ref/train_processed_finecopes_ref_30k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fc953ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>objects_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2373626</td>\n",
       "      <td>The dish, positioned to the right of the gray ...</td>\n",
       "      <td>[{'object_id': '3725798', 'names': ['dish'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285719</td>\n",
       "      <td>On the left side of the metal and black fence,...</td>\n",
       "      <td>[{'object_id': '4401457', 'names': ['bike'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2383125</td>\n",
       "      <td>The post that is to the left of the car that i...</td>\n",
       "      <td>[{'object_id': '534946', 'names': ['car'], 'h'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2349520</td>\n",
       "      <td>The clock that is to the left of the tower tha...</td>\n",
       "      <td>[{'object_id': '3903072', 'names': ['roof'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2352548</td>\n",
       "      <td>The white keyboard positioned to the right of ...</td>\n",
       "      <td>[{'object_id': '1807596', 'names': ['computer'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>2410239</td>\n",
       "      <td>On the white glass, the white napkins are neat...</td>\n",
       "      <td>[{'object_id': '222683', 'names': ['bowl'], 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>2321990</td>\n",
       "      <td>The orange and old chair that is to the right ...</td>\n",
       "      <td>[{'object_id': '3341883', 'names': ['bandana']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>2367927</td>\n",
       "      <td>A chair made of the same materials as the tabl...</td>\n",
       "      <td>[{'object_id': '749365', 'names': ['chair'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>2355072</td>\n",
       "      <td>The yellow train that is near the green and ta...</td>\n",
       "      <td>[{'object_id': '833081', 'names': ['building']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>2343300</td>\n",
       "      <td>On the left side of the white flower, which it...</td>\n",
       "      <td>[{'object_id': '3640816', 'names': ['napkins']...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                             answer  \\\n",
       "0       2373626  The dish, positioned to the right of the gray ...   \n",
       "1        285719  On the left side of the metal and black fence,...   \n",
       "2       2383125  The post that is to the left of the car that i...   \n",
       "3       2349520  The clock that is to the left of the tower tha...   \n",
       "4       2352548  The white keyboard positioned to the right of ...   \n",
       "...         ...                                                ...   \n",
       "29995   2410239  On the white glass, the white napkins are neat...   \n",
       "29996   2321990  The orange and old chair that is to the right ...   \n",
       "29997   2367927  A chair made of the same materials as the tabl...   \n",
       "29998   2355072  The yellow train that is near the green and ta...   \n",
       "29999   2343300  On the left side of the white flower, which it...   \n",
       "\n",
       "                                            objects_info  \n",
       "0      [{'object_id': '3725798', 'names': ['dish'], '...  \n",
       "1      [{'object_id': '4401457', 'names': ['bike'], '...  \n",
       "2      [{'object_id': '534946', 'names': ['car'], 'h'...  \n",
       "3      [{'object_id': '3903072', 'names': ['roof'], '...  \n",
       "4      [{'object_id': '1807596', 'names': ['computer'...  \n",
       "...                                                  ...  \n",
       "29995  [{'object_id': '222683', 'names': ['bowl'], 'h...  \n",
       "29996  [{'object_id': '3341883', 'names': ['bandana']...  \n",
       "29997  [{'object_id': '749365', 'names': ['chair'], '...  \n",
       "29998  [{'object_id': '833081', 'names': ['building']...  \n",
       "29999  [{'object_id': '3640816', 'names': ['napkins']...  \n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5a5c5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dish, positioned to the right of the gray bowl and further down from the red strawberry, rests elegantly on the table.'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df[\"answer\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d9c81480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'object_id': '3725798', 'names': ['dish'], 'h': 131, 'w': 174, 'y': 1, 'x': 325}, {'object_id': '2428018', 'names': ['gray bowl'], 'h': 159, 'w': 179, 'y': 0, 'x': 0}, {'object_id': '2235351', 'names': ['red strawberry'], 'h': 50, 'w': 65, 'y': 73, 'x': 290}]\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df[\"objects_info\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee15026",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25318297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92d8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = batch_df[~batch_df[\"local_path\"].isin(poc_df[\"local_path\"].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86245499",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_batch_1_15000_gcp_upload_urs.csv\")\n",
    "prompt_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/llava_gen_des_train.csv\").sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a465e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_path</th>\n",
       "      <th>gcs_uri</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "      <td>gs://train_data_vision_1/coco_batch_1_15000/CO...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "      <td>gs://train_data_vision_1/coco_batch_1_15000/CO...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          local_path  \\\n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...   \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...   \n",
       "\n",
       "                                             gcs_uri  error  \n",
       "0  gs://train_data_vision_1/coco_batch_1_15000/CO...    NaN  \n",
       "1  gs://train_data_vision_1/coco_batch_1_15000/CO...    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1326032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8803</th>\n",
       "      <td>COCO_train2014_000000559900.jpg</td>\n",
       "      <td>This indoor scene captures two young boys enjo...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>8803</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>COCO_train2014_000000463308.jpg</td>\n",
       "      <td>This outdoor scene captures a dynamic moment d...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>8660</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_id  \\\n",
       "8803  COCO_train2014_000000559900.jpg   \n",
       "8660  COCO_train2014_000000463308.jpg   \n",
       "\n",
       "                                                 answer  \\\n",
       "8803  This indoor scene captures two young boys enjo...   \n",
       "8660  This outdoor scene captures a dynamic moment d...   \n",
       "\n",
       "                                   question  question_id  \\\n",
       "8803  Please describe this image in detail.         8803   \n",
       "8660  Please describe this image in detail.         8660   \n",
       "\n",
       "                                             image_path  \n",
       "8803  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "8660  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e78243",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f7260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c19e1cf0",
   "metadata": {},
   "source": [
    "# batch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0393d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(question, ans):\n",
    "  prompt = f\"\"\"\n",
    "You are given an **image** and its **caption** describing the visual scene.\n",
    "Your job is to **analyze the caption** and extract all **meaningful, semantically important words** that can be visually grounded in the image.\n",
    "Each extracted word must be categorized into one of the following classes:\n",
    "\n",
    "* **objects** (car, tree, person, lamp)\n",
    "* **attributes** (red, shiny, tall)\n",
    "* **relations** (on, under, behind, next_to)\n",
    "* **actions** (running, holding, sitting)\n",
    "* **count** (two, many, three)\n",
    "* **scene/context** (beach, kitchen, street)\n",
    "* **decision tokens** (yes, no, true, false, present, absent, exist, not, visible, correct)\n",
    "\n",
    "For every detected word, provide a structured JSON entry with its **category, word, related object IDs, and bounding box** coordinates.\n",
    "If the word refers to a hallucinated (nonexistent) visual concept, set its bounding box as `[0, 0, 0, 0]`.\n",
    "\n",
    "Note: always use image scale as 1000 * 1000 for bounding box coordinates.\n",
    "---\n",
    "\n",
    "### ðŸ§¾ **Expected Output Format**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"objects\": [\n",
    "    {{\"id\": 1, \"word\": \"dog\", \"bounding_box\": [120, 150, 260, 310]}},\n",
    "    {{\"id\": 2, \"word\": \"ball\", \"bounding_box\": [400, 230, 460, 280]}}\n",
    "  ],\n",
    "  \"attributes\": [\n",
    "    {{\"word\": \"brown\", \"objects_involved\": [1]}},\n",
    "    {{\"word\": \"round\", \"objects_involved\": [2]}}\n",
    "  ],\n",
    "  \"relations\": [\n",
    "    {{\"word\": \"holding\", \"objects_involved\": [1, 2]}},\n",
    "   {{\"word\": \"on\", \"objects_involved\": [1, 2]}}\n",
    "  ],\n",
    "  \"count\": [\n",
    "    {{\"word\": \"two\", \"objects_involved\": [1, 2]}}\n",
    "  ],\n",
    "  \"scene\": [\n",
    "    {{\"word\": \"park\", \"bounding_box\": [0, 0, 640, 480]}}\n",
    "  ],\n",
    "  \"decision_tokens\": [\n",
    "   {{\"word\": \"visible\", \"objects_involved\": [1]}}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "Inputs:\n",
    "<attached image>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Caption: {ans}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "  return prompt\n",
    "\n",
    "all_res = []\n",
    "for inx, row in prompt_df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    ans  = row[\"answer\"]\n",
    "    PROMPT = get_prompt(question, ans)\n",
    "    img_path = row[\"image_path\"]\n",
    "    uri = final_df[final_df[\"local_path\"] == img_path][\"gcs_uri\"].iloc[0]\n",
    "    \n",
    "    res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "                      \"generationConfig\": {\"temperature\": 0, \"topP\": 1, \"maxOutputTokens\": 5000,\"thinking_config\":{\"thinking_budget\":1000}}}}\n",
    "    all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ac5e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9e86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PROMPT = \"\"\"\n",
    "# You are a specialist in rich and precise scene understanding.\n",
    "# Given an input image, generate a comprehensive, contextually aware, and fluent description that captures all key visual elements, their relationships, emotions, and possible context or story.\n",
    "\n",
    "# Your description should go beyond short captions â€” it must resemble a paragraph of visual storytelling that includes:\n",
    "\n",
    "# Scene type: indoor/outdoor, environment, lighting, time of day\n",
    "# Objects and entities: names, counts, shapes, colors, materials\n",
    "# Actions and interactions: what the people or objects are doing\n",
    "# Spatial layout: foreground, background, relative positions\n",
    "# Emotions or atmosphere: tone, mood, aesthetics\n",
    "# Possible context: what might be happening or implied by the scene\n",
    "\n",
    "# Avoid generic or repetitive statements. Be vivid, factual, and coherent. Use natural language instead of bullet points.\n",
    "\n",
    "# Output JSON format:\n",
    "# {image_description: <full attached image description>}\n",
    "# \"\"\"\n",
    "\n",
    "# all_res = []\n",
    "# for uri in df[\"gcs_uri\"].tolist():\n",
    "#     res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "#                       \"generationConfig\": {\"temperature\": 0.6, \"topP\": 1, \"maxOutputTokens\": 1000,\"thinking_config\":{\"thinking_budget\":0}}}}\n",
    "#     all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a069954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt(question, ans_1, ans_2):\n",
    "#   prompt = f\"\"\"\n",
    "# You are given an **image** and **two questionâ€“answer (QA) pairs** related to that image.\n",
    "# Your job is to analyze each QA pair separately and return **two lists of single words (no phrases)**:\n",
    "\n",
    "# 1. **`hallucination_candidates`** â€” all meaningful words that are semantically important for hallucination detection, belonging to categories like:\n",
    "\n",
    "#    * **objects** (car, tree, person, lamp),\n",
    "#    * **attributes** (red, shiny, tall),\n",
    "#    * **relations** (on, under, behind, next_to),\n",
    "#    * **actions** (running, holding, sitting),\n",
    "#    * **count** (two, many, three),\n",
    "#    * **scene/context** (beach, kitchen, street).\n",
    "#    * **Decision tokens** yes, no, true, false, present, absent, exist, not, visible, correct\n",
    "\n",
    "#    Include only **specific, content-bearing words** that could be visually verifiable or falsifiable from the image.\n",
    "#    Do **not** include stopwords, determiners, or abstract terms.\n",
    "\n",
    "# 2. **`hallucinated_words`** â€” a subset of the above list that are **not visually supported** or are **contradicted** by the image content (i.e., hallucinated terms).\n",
    "\n",
    "# **Inputs:**\n",
    "\n",
    "# <attached image>\n",
    "\n",
    "# qa_pair_1:\n",
    "# Question: {question}\n",
    "# Answer: {ans_1}\n",
    "\n",
    "# qa_pair_2:\n",
    "# Question: {question}\n",
    "# Answer: {ans_2}\n",
    "\n",
    "\n",
    "# **Output format (strict JSON per QA pair):**\n",
    "\n",
    "# ```json\n",
    "# {{\n",
    "#   \"qa_pair_1\": {{\n",
    "#     \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "#     \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "#   }},\n",
    "#   \"qa_pair_2\": {{\n",
    "#     \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "#     \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "#   }}\n",
    "# }}\n",
    "# ```\n",
    "# Note: extract \"hallucination_candidates\" and \"hallucinated_words\" from the Answer strictly not from the Question\n",
    "\n",
    "# Ensure both lists contain **only lowercase single words** and are **deduplicated**.\n",
    "# Focus on **important, visually grounded words** â€” not all tokens.\n",
    "\n",
    "# ---\n",
    "# \"\"\"\n",
    "#   return prompt\n",
    "\n",
    "# all_res = []\n",
    "# for inx, row in prompt_df.iterrows():\n",
    "#     question = row[\"prompt\"].replace(\"<image>\", \"\")\n",
    "#     nh_ans  = row[\"source_text\"]\n",
    "#     h_ans = row[\"hallucinated_text\"]\n",
    "#     PROMPT = get_prompt(question, nh_ans, h_ans)\n",
    "#     img_path = f\"/Data2/Arun-UAV/NLP/vision_halu/visual_genome/target_images/{row['image_id']}.jpg\"\n",
    "#     uri = df[df[\"local_path\"] == img_path][\"gcs_uri\"].iloc[0]\n",
    "\n",
    "\n",
    "#     res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "#                       \"generationConfig\": {\"temperature\": 0.6, \"topP\": 1, \"maxOutputTokens\": 1000,\"thinking_config\":{\"thinking_budget\":0}}}}\n",
    "#     all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8b909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt(question, ans_1, ans_2):\n",
    "#   prompt = f\"\"\"\n",
    "# You are given an **image** and **two captions (descriptions)** related to that image.\n",
    "# Your job is to analyze each caption separately and return **two lists of single words (no phrases)**:\n",
    "\n",
    "# 1. **`hallucination_candidates`** â€” all meaningful words that are semantically important for hallucination detection, belonging to categories like:\n",
    "\n",
    "#    * **objects** (car, tree, person, lamp),\n",
    "#    * **attributes** (red, shiny, tall),\n",
    "#    * **relations** (on, under, behind, next_to),\n",
    "#    * **actions** (running, holding, sitting),\n",
    "#    * **count** (two, many, three),\n",
    "#    * **scene/context** (beach, kitchen, street).\n",
    "#    * **Decision tokens** yes, no, true, false, present, absent, exist, not, visible, correct\n",
    "\n",
    "#    Include only **specific, content-bearing words** that could be visually verifiable or falsifiable from the image.\n",
    "#    Do **not** include stopwords, determiners, or abstract terms.\n",
    "\n",
    "# 2. **`hallucinated_words`** â€” a subset of the above list that are **not visually supported** or are **contradicted** by the image content (i.e., hallucinated terms).\n",
    "\n",
    "# **Inputs:**\n",
    "\n",
    "# <attached image>\n",
    "\n",
    "# caption_pair_1:\n",
    "# Question: {question}\n",
    "# Caption: {ans_1}\n",
    "\n",
    "# caption_pair_2:\n",
    "# Question: {question}\n",
    "# Caption: {ans_2}\n",
    "\n",
    "\n",
    "# **Output format (strict JSON per caption pair):**\n",
    "\n",
    "# ```json\n",
    "# {{\n",
    "#   \"caption_pair_1\": {{\n",
    "#     \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "#     \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "#   }},\n",
    "#   \"caption_pair_2\": {{\n",
    "#     \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "#     \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "#   }}\n",
    "# }}\n",
    "# ```\n",
    "# Note: extract \"hallucination_candidates\" and \"hallucinated_words\" from the Answer strictly not from the Question\n",
    "\n",
    "# Ensure both lists contain **only lowercase single words** and are **deduplicated**.\n",
    "# Focus on **important, visually grounded words** â€” not all tokens.\n",
    "\n",
    "# ---\n",
    "# \"\"\"\n",
    "#   return prompt\n",
    "\n",
    "# all_res = []\n",
    "# for inx, row in prompt_df.iterrows():\n",
    "#     question = row[\"prompt\"].replace(\"<image>\", \"\")\n",
    "#     nh_ans  = row[\"source_text\"]\n",
    "#     h_ans = row[\"hallucinated_text\"]\n",
    "#     PROMPT = get_prompt(question, nh_ans, h_ans)\n",
    "#     img_path = f\"/Data2/Arun-UAV/NLP/vision_halu/visual_genome/target_images/{row['image_id']}.jpg\"\n",
    "#     uri = df[df[\"local_path\"] == img_path][\"gcs_uri\"].iloc[0]\n",
    "\n",
    "#     res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "#                       \"generationConfig\": {\"temperature\": 0, \"topP\": 1, \"maxOutputTokens\": 2000,\"thinking_config\":{\"thinking_budget\":1000}}}}\n",
    "#     all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e0a8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res)\n",
    "df.to_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29222894",
   "metadata": {},
   "source": [
    "# Uploading files to gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d038d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(local_path: str, gcs_uri: str):\n",
    "    \"\"\"\n",
    "    Upload a local file to a target GCS URI.\n",
    "\n",
    "    Args:\n",
    "        local_path (str): Local file path to upload.\n",
    "        gcs_uri (str): Target GCS URI like 'gs://my-bucket/path/to/upload.txt'\n",
    "        service_account_path (str): Path to GCP service account JSON.\n",
    "    \"\"\"\n",
    "    if not gcs_uri.startswith(\"gs://\"):\n",
    "        raise ValueError(\"Invalid GCS URI. Must start with gs://\")\n",
    "\n",
    "    parts = gcs_uri[5:].split(\"/\", 1)\n",
    "    bucket_name = parts[0]\n",
    "    blob_name = parts[1]\n",
    "\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(local_path)\n",
    "\n",
    "    print(f\"âœ… Uploaded {local_path} â†’ {gcs_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cf3a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Uploaded /Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl â†’ gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\n"
     ]
    }
   ],
   "source": [
    "upload_to_gcs(local_path=\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl\", gcs_uri = \"gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf87eca",
   "metadata": {},
   "source": [
    "# start batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9efc5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: projects/358874265041/locations/us-central1/batchPredictionJobs/8274965141032796160\n",
      "Job state: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import CreateBatchJobConfig, JobState, HttpOptions\n",
    "output_uri = \"gs://train_data_vision_1/gemini_batch_info/\"\n",
    "\n",
    "# See the documentation: https://googleapis.github.io/python-genai/genai.html#genai.batches.Batches.create\n",
    "job = gen_client.batches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    src=\"gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\",\n",
    "    config=CreateBatchJobConfig(dest=output_uri),\n",
    ")\n",
    "print(f\"Job name: {job.name}\")\n",
    "print(f\"Job state: {job.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9126c060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_info = gen_client.batches.get(name=job.name)\n",
    "job_info.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf8435",
   "metadata": {},
   "source": [
    "# download batch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c16b2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gcs(gcs_uri: str, local_path: str):\n",
    "    \"\"\"\n",
    "    Download a file from GCS based on its gs:// URI.\n",
    "\n",
    "    Args:\n",
    "        gcs_uri (str): GCS URI like 'gs://my-bucket/path/to/file.txt'\n",
    "        local_path (str): Path to store the downloaded file locally.\n",
    "    \"\"\"\n",
    "    # Parse bucket and blob name\n",
    "    if not gcs_uri.startswith(\"gs://\"):\n",
    "        raise ValueError(\"Invalid GCS URI. Must start with gs://\")\n",
    "\n",
    "    parts = gcs_uri[5:].split(\"/\", 1)\n",
    "    bucket_name = parts[0]\n",
    "    blob_name = parts[1]\n",
    "    \n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    blob.download_to_filename(local_path)\n",
    "\n",
    "    print(f\"âœ… Downloaded {gcs_uri} â†’ {local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7622509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Downloaded gs://train_data_vision_1/gemini_batch_info/prediction-model-2025-10-28T14:36:23.912008Z/predictions.jsonl â†’ /Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\n"
     ]
    }
   ],
   "source": [
    "download_from_gcs(gcs_uri=\"gs://train_data_vision_1/gemini_batch_info/prediction-model-2025-10-28T14:36:23.912008Z/predictions.jsonl\", local_path =\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a24bb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.read_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5b539e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_2_bbox(target_df, objects_df):\n",
    "    bbox_map = {}\n",
    "    for row in target_df.iterrows():\n",
    "        word = row[1][\"word\"]\n",
    "        obj_ids = row[1][\"objects_involved\"]\n",
    "        bbox = objects_df[objects_df[\"id\"].isin(obj_ids)][\"bounding_box\"].to_list()\n",
    "        bbox_map[word] = bbox\n",
    "    return bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0270f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [01:58, 84.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_res = []\n",
    "error = 0\n",
    "for inx, row in tqdm(pred_data.iterrows()):\n",
    "    try:\n",
    "        img_name = row[\"request\"][\"contents\"][0][\"parts\"][1][\"fileData\"][\"fileUri\"].split(\"/\")[-1]\n",
    "        res = eval(row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].replace(\"json\", \"\").strip(\"```\"))\n",
    "        objects_df = pd.DataFrame(res[\"objects\"])\n",
    "        \n",
    "        target_words = {i:j for i, j in zip(objects_df[\"word\"], objects_df[\"bounding_box\"])}\n",
    "        \n",
    "        try:\n",
    "            attr_df = pd.DataFrame(res[\"attributes\"])\n",
    "            if attr_df.shape[0]>0:\n",
    "                attr_df = attr_df[attr_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                att_words = get_word_2_bbox(attr_df, objects_df)\n",
    "                target_words.update(att_words)\n",
    "\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "            \n",
    "        try:\n",
    "            rel_df = pd.DataFrame(res[\"relations\"])\n",
    "            if rel_df.shape[0]>0:\n",
    "                rel_df = rel_df[rel_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                rel_words = get_word_2_bbox(rel_df, objects_df)\n",
    "                target_words.update(rel_words)\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "        \n",
    "        try:\n",
    "            count_df = pd.DataFrame(res[\"count\"])\n",
    "            if count_df.shape[0]>0:\n",
    "                count_df = count_df[count_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                count_words = get_word_2_bbox(count_df, objects_df)\n",
    "                target_words.update(count_words)\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "        \n",
    "        try:\n",
    "            scene_df = pd.DataFrame(res[\"scene\"])\n",
    "            if scene_df.shape[0]>0:\n",
    "                scene_words = {i:j for i, j in zip(scene_df[\"word\"], scene_df[\"bounding_box\"])}\n",
    "                target_words.update(scene_words)\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "\n",
    "        try:\n",
    "            decision_df = pd.DataFrame(res[\"decision_tokens\"])\n",
    "            if decision_df.shape[0]>0:\n",
    "                decision_df = decision_df[decision_df[\"objects_involved\"].apply(lambda x: len(x)!=0)]\n",
    "                decision_words = get_word_2_bbox(decision_df, objects_df)\n",
    "            target_words.update(decision_words)\n",
    "        except Exception as e:\n",
    "            error += 1\n",
    "        \n",
    "        all_res.append({\"image_name\": img_name, \"word_2_bbox\": target_words})\n",
    "    \n",
    "    except Exception as e:\n",
    "        error += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ab394aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_res_df = pd.DataFrame(all_res)\n",
    "columns=[\"image_id\", \"word_2_bbox\"]\n",
    "clean_res_df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae50e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df = pd.merge(clean_res_df, prompt_df, on = \"image_id\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "70534e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>word_2_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000507721.jpg</td>\n",
       "      <td>{'grass': [0, 400, 1000, 1000], 'structure': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000560217.jpg</td>\n",
       "      <td>{'cat': [150, 180, 850, 800], 'film stock labe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id  \\\n",
       "0  COCO_train2014_000000507721.jpg   \n",
       "1  COCO_train2014_000000560217.jpg   \n",
       "\n",
       "                                         word_2_bbox  \n",
       "0  {'grass': [0, 400, 1000, 1000], 'structure': [...  \n",
       "1  {'cat': [150, 180, 850, 800], 'film stock labe...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33e1026b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8803</th>\n",
       "      <td>COCO_train2014_000000559900.jpg</td>\n",
       "      <td>This indoor scene captures two young boys enjo...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>8803</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>COCO_train2014_000000463308.jpg</td>\n",
       "      <td>This outdoor scene captures a dynamic moment d...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>8660</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_id  \\\n",
       "8803  COCO_train2014_000000559900.jpg   \n",
       "8660  COCO_train2014_000000463308.jpg   \n",
       "\n",
       "                                                 answer  \\\n",
       "8803  This indoor scene captures two young boys enjo...   \n",
       "8660  This outdoor scene captures a dynamic moment d...   \n",
       "\n",
       "                                   question  question_id  \\\n",
       "8803  Please describe this image in detail.         8803   \n",
       "8660  Please describe this image in detail.         8660   \n",
       "\n",
       "                                             image_path  \n",
       "8803  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "8660  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df22d5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>word_2_bbox</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000507721.jpg</td>\n",
       "      <td>{'grass': [0, 400, 1000, 1000], 'structure': [...</td>\n",
       "      <td>This outdoor scene captures a rustic, rural la...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>5616</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000560217.jpg</td>\n",
       "      <td>{'cat': [150, 180, 850, 800], 'film stock labe...</td>\n",
       "      <td>This outdoor, dimly lit scene, captured likely...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>1966</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id  \\\n",
       "0  COCO_train2014_000000507721.jpg   \n",
       "1  COCO_train2014_000000560217.jpg   \n",
       "\n",
       "                                         word_2_bbox  \\\n",
       "0  {'grass': [0, 400, 1000, 1000], 'structure': [...   \n",
       "1  {'cat': [150, 180, 850, 800], 'film stock labe...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  This outdoor scene captures a rustic, rural la...   \n",
       "1  This outdoor, dimly lit scene, captured likely...   \n",
       "\n",
       "                                question  question_id  \\\n",
       "0  Please describe this image in detail.         5616   \n",
       "1  Please describe this image in detail.         1966   \n",
       "\n",
       "                                          image_path  \n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4e2bded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/evidence_head_train_datasets/coco_long_captions/coco_img_des_10k_bb_annot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "21243266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>word_2_bbox</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000507721.jpg</td>\n",
       "      <td>{'grass': [0, 400, 1000, 1000], 'structure': [...</td>\n",
       "      <td>This outdoor scene captures a rustic, rural la...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>5616</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000560217.jpg</td>\n",
       "      <td>{'cat': [150, 180, 850, 800], 'film stock labe...</td>\n",
       "      <td>This outdoor, dimly lit scene, captured likely...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>1966</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id  \\\n",
       "0  COCO_train2014_000000507721.jpg   \n",
       "1  COCO_train2014_000000560217.jpg   \n",
       "\n",
       "                                         word_2_bbox  \\\n",
       "0  {'grass': [0, 400, 1000, 1000], 'structure': [...   \n",
       "1  {'cat': [150, 180, 850, 800], 'film stock labe...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  This outdoor scene captures a rustic, rural la...   \n",
       "1  This outdoor, dimly lit scene, captured likely...   \n",
       "\n",
       "                                question  question_id  \\\n",
       "0  Please describe this image in detail.         5616   \n",
       "1  Please describe this image in detail.         1966   \n",
       "\n",
       "                                          image_path  \n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175add22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d15f24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_qa_pairs(text: str):\n",
    "    \"\"\"\n",
    "    Extracts qa_pair_1 and qa_pair_2 (Question and Answer) from multiple prompts\n",
    "    having the same structure.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text containing multiple prompt blocks.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of dictionaries, each containing qa_pair_1 and qa_pair_2 with Q/A.\n",
    "    \"\"\"\n",
    "    # Pattern to match both caption_pair_1 and caption_pair_2 blocks\n",
    "    pattern = re.compile(\n",
    "        r'caption_pair_1:\\s*Question:\\s*(?P<q1>.*?)\\s*Caption:\\s*(?P<a1>.*?)\\s*'\n",
    "        r'caption_pair_2:\\s*Question:\\s*(?P<q2>.*?)\\s*Caption:\\s*(?P<a2>.*?)(?=\\n\\s*\\n|$)',\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for match in pattern.finditer(text):\n",
    "        qa_dict = {\n",
    "            \"caption_pair_1\": {\n",
    "                \"Question\": match.group(\"q1\").strip(),\n",
    "                \"Caption\": match.group(\"a1\").strip()\n",
    "            },\n",
    "            \"caption_pair_2\": {\n",
    "                \"Question\": match.group(\"q2\").strip(),\n",
    "                \"Caption\": match.group(\"a2\").strip()\n",
    "            }\n",
    "        }\n",
    "        results.append(qa_dict)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6c523ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "error = 0\n",
    "for inx, row in pred_data.iterrows():\n",
    "    try:\n",
    "        img_name = row[\"request\"][\"contents\"][0][\"parts\"][1][\"fileData\"][\"fileUri\"].split(\"/\")[-1]\n",
    "        qa = extract_qa_pairs(row[\"request\"][\"contents\"][0][\"parts\"][0][\"text\"].split(\"<attached image>\")[-1].split(\"**Output format\")[0])\n",
    "\n",
    "        question = qa[0][\"caption_pair_1\"]['Question']\n",
    "        answer_1 = qa[0][\"caption_pair_1\"]['Caption']\n",
    "        answer_2 = qa[0][\"caption_pair_2\"]['Caption']\n",
    "\n",
    "        img_dec = row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        c_img_dec = eval(img_dec.replace(\"json\", \"\").strip(\"```\"))\n",
    "\n",
    "        all_res.append([img_name, question, answer_1,  c_img_dec[\"caption_pair_1\"][\"hallucination_candidates\"], c_img_dec[\"caption_pair_1\"][\"hallucinated_words\"]])\n",
    "        all_res.append([img_name, question, answer_2,  c_img_dec[\"caption_pair_2\"][\"hallucination_candidates\"], c_img_dec[\"caption_pair_2\"][\"hallucinated_words\"]])\n",
    "    except Exception as e:\n",
    "        error += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0368eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84ce5e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27986, 5)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4fea5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns = [\"image_path\", \"question\", \"answer\", \"candidates\", \"hallucination_candidates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76122e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"/Data2/Arun-UAV/NLP/vision_halu/haloc/haloc_extension/caption/gemini_labeled_28k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a1e4a39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>candidates</th>\n",
       "      <th>hallucination_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2381430.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see one woman is holdin...</td>\n",
       "      <td>[one, woman, holding, bat, playing, tennis, ba...</td>\n",
       "      <td>[bat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381430.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see one woman is holdin...</td>\n",
       "      <td>[one, woman, holding, bat, playing, tennis, fr...</td>\n",
       "      <td>[bat, frisbee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2378712.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see a bus on road and a...</td>\n",
       "      <td>[bus, road, footpath, statue, person, building...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2378712.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see a bus on road and a...</td>\n",
       "      <td>[bus, road, footpath, statue, person, building...</td>\n",
       "      <td>[trains]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2379616.jpg</td>\n",
       "      <td>Describe the following image.</td>\n",
       "      <td>There is a person standing on a bicycle and th...</td>\n",
       "      <td>[person, standing, bicycle, car, beside, sitti...</td>\n",
       "      <td>[standing, sitting, bike, left, corner, few, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>2398155.jpg</td>\n",
       "      <td>What do you see happening in this image?</td>\n",
       "      <td>An older woman in a black, white, and gray shi...</td>\n",
       "      <td>[older, woman, black, white, gray, shirt, pant...</td>\n",
       "      <td>[gray, apron, five, stove]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27982</th>\n",
       "      <td>2347016.jpg</td>\n",
       "      <td>Write a detailed description of the given image.</td>\n",
       "      <td>In this picture we can see a black color cat s...</td>\n",
       "      <td>[black, cat, sitting, laptop, on, book, pencil...</td>\n",
       "      <td>[cd, remote]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27983</th>\n",
       "      <td>2347016.jpg</td>\n",
       "      <td>Write a detailed description of the given image.</td>\n",
       "      <td>In this picture we can see a black color cat s...</td>\n",
       "      <td>[black, cat, sitting, laptop, on, book, pencil...</td>\n",
       "      <td>[left, cd, remote, cup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27984</th>\n",
       "      <td>2316274.jpg</td>\n",
       "      <td>Can you elaborate on the elements of the pictu...</td>\n",
       "      <td>In this image there is a dog standing in a roo...</td>\n",
       "      <td>[dog, standing, room, carpet, books, tables, c...</td>\n",
       "      <td>[standing, toys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27985</th>\n",
       "      <td>2316274.jpg</td>\n",
       "      <td>Can you elaborate on the elements of the pictu...</td>\n",
       "      <td>In this image there is a dog standing in a roo...</td>\n",
       "      <td>[dog, standing, room, carpet, books, tables, c...</td>\n",
       "      <td>[standing, toys, bed, frisbees, scattered]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27986 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_path                                           question  \\\n",
       "0      2381430.jpg    What do you think is going on in this snapshot?   \n",
       "1      2381430.jpg    What do you think is going on in this snapshot?   \n",
       "2      2378712.jpg    What do you think is going on in this snapshot?   \n",
       "3      2378712.jpg    What do you think is going on in this snapshot?   \n",
       "4      2379616.jpg                      Describe the following image.   \n",
       "...            ...                                                ...   \n",
       "27981  2398155.jpg           What do you see happening in this image?   \n",
       "27982  2347016.jpg   Write a detailed description of the given image.   \n",
       "27983  2347016.jpg   Write a detailed description of the given image.   \n",
       "27984  2316274.jpg  Can you elaborate on the elements of the pictu...   \n",
       "27985  2316274.jpg  Can you elaborate on the elements of the pictu...   \n",
       "\n",
       "                                                  answer  \\\n",
       "0      In this picture we can see one woman is holdin...   \n",
       "1      In this picture we can see one woman is holdin...   \n",
       "2      In this picture we can see a bus on road and a...   \n",
       "3      In this picture we can see a bus on road and a...   \n",
       "4      There is a person standing on a bicycle and th...   \n",
       "...                                                  ...   \n",
       "27981  An older woman in a black, white, and gray shi...   \n",
       "27982  In this picture we can see a black color cat s...   \n",
       "27983  In this picture we can see a black color cat s...   \n",
       "27984  In this image there is a dog standing in a roo...   \n",
       "27985  In this image there is a dog standing in a roo...   \n",
       "\n",
       "                                              candidates  \\\n",
       "0      [one, woman, holding, bat, playing, tennis, ba...   \n",
       "1      [one, woman, holding, bat, playing, tennis, fr...   \n",
       "2      [bus, road, footpath, statue, person, building...   \n",
       "3      [bus, road, footpath, statue, person, building...   \n",
       "4      [person, standing, bicycle, car, beside, sitti...   \n",
       "...                                                  ...   \n",
       "27981  [older, woman, black, white, gray, shirt, pant...   \n",
       "27982  [black, cat, sitting, laptop, on, book, pencil...   \n",
       "27983  [black, cat, sitting, laptop, on, book, pencil...   \n",
       "27984  [dog, standing, room, carpet, books, tables, c...   \n",
       "27985  [dog, standing, room, carpet, books, tables, c...   \n",
       "\n",
       "                                hallucination_candidates  \n",
       "0                                                  [bat]  \n",
       "1                                         [bat, frisbee]  \n",
       "2                                                     []  \n",
       "3                                               [trains]  \n",
       "4      [standing, sitting, bike, left, corner, few, r...  \n",
       "...                                                  ...  \n",
       "27981                         [gray, apron, five, stove]  \n",
       "27982                                       [cd, remote]  \n",
       "27983                            [left, cd, remote, cup]  \n",
       "27984                                   [standing, toys]  \n",
       "27985         [standing, toys, bed, frisbees, scattered]  \n",
       "\n",
       "[27986 rows x 5 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d45f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a473d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for inx, row in pred_data.iterrows():\n",
    "    try:\n",
    "        img_name = row[\"request\"][\"contents\"][0][\"parts\"][1][\"fileData\"][\"fileUri\"].split(\"/\")[-1]\n",
    "        img_dec = row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        c_img_dec = eval(img_dec.replace(\"json\", \"\").strip(\"```\"))[\"image_description\"]\n",
    "        all_res.append({\"image\": img_name, \"description\": c_img_dec})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {inx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cd58247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eb243a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000467172.jpg</td>\n",
       "      <td>The indoor scene captures two domestic cats ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000328023.jpg</td>\n",
       "      <td>This outdoor scene captures a lively moment in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image  \\\n",
       "0  COCO_train2014_000000467172.jpg   \n",
       "1  COCO_train2014_000000328023.jpg   \n",
       "\n",
       "                                         description  \n",
       "0  The indoor scene captures two domestic cats ex...  \n",
       "1  This outdoor scene captures a lively moment in...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ebb6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_poc_df  = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_5000_train_with_gemini_des.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef526b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_5000_train_with_gemini_des.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b50f8312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000078572.jpg</td>\n",
       "      <td>This outdoor scene captures a bustling street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000401963.jpg</td>\n",
       "      <td>The image captures an outdoor scene under brig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image  \\\n",
       "0  COCO_train2014_000000078572.jpg   \n",
       "1  COCO_train2014_000000401963.jpg   \n",
       "\n",
       "                                         description  \n",
       "0  This outdoor scene captures a bustling street ...  \n",
       "1  The image captures an outdoor scene under brig...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_poc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ed88430",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_15k = pd.concat([old_poc_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f44e69ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14579"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_15k[\"image\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7913014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_15k.to_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_batch_1_15000_train_with_gemini_des.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0d87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
