{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552c53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from google import genai\n",
    "\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "SERVICE_ACCOUNT_FILE = \"/Data2/Arun-UAV/NLP/new_cloud_coount.json\"\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)\n",
    "\n",
    "client = storage.Client(credentials=credentials)\n",
    "\n",
    "gen_client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project='third-apex-476512-a7',   # or set directly\n",
    "    location='us-central1',    # or set directly, e.g. \"us-central1\"\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14b6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/llava_gen_des_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc953ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000078572.jpg</td>\n",
       "      <td>This outdoor scene captures a bustling street ...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>0</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000401963.jpg</td>\n",
       "      <td>The image captures an outdoor scene under brig...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>1</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COCO_train2014_000000494297.jpg</td>\n",
       "      <td>This indoor scene, captured in what appears to...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>2</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COCO_train2014_000000110381.jpg</td>\n",
       "      <td>This image captures an intimate, rustic indoor...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>3</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COCO_train2014_000000550369.jpg</td>\n",
       "      <td>The scene unfolds in what appears to be an ind...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>4</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14574</th>\n",
       "      <td>COCO_train2014_000000191149.jpg</td>\n",
       "      <td>This outdoor scene captures a bright, sunny da...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>14574</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14575</th>\n",
       "      <td>COCO_train2014_000000217433.jpg</td>\n",
       "      <td>This image captures an outdoor rooftop terrace...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>14575</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576</th>\n",
       "      <td>COCO_train2014_000000080428.jpg</td>\n",
       "      <td>This outdoor scene depicts a harsh, wintry lan...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>14576</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14577</th>\n",
       "      <td>COCO_train2014_000000485002.jpg</td>\n",
       "      <td>This outdoor scene captures a vibrant, sunny g...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>14577</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14578</th>\n",
       "      <td>COCO_train2014_000000419627.jpg</td>\n",
       "      <td>The outdoor scene captures a dynamic moment in...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>14578</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14579 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_id  \\\n",
       "0      COCO_train2014_000000078572.jpg   \n",
       "1      COCO_train2014_000000401963.jpg   \n",
       "2      COCO_train2014_000000494297.jpg   \n",
       "3      COCO_train2014_000000110381.jpg   \n",
       "4      COCO_train2014_000000550369.jpg   \n",
       "...                                ...   \n",
       "14574  COCO_train2014_000000191149.jpg   \n",
       "14575  COCO_train2014_000000217433.jpg   \n",
       "14576  COCO_train2014_000000080428.jpg   \n",
       "14577  COCO_train2014_000000485002.jpg   \n",
       "14578  COCO_train2014_000000419627.jpg   \n",
       "\n",
       "                                                  answer  \\\n",
       "0      This outdoor scene captures a bustling street ...   \n",
       "1      The image captures an outdoor scene under brig...   \n",
       "2      This indoor scene, captured in what appears to...   \n",
       "3      This image captures an intimate, rustic indoor...   \n",
       "4      The scene unfolds in what appears to be an ind...   \n",
       "...                                                  ...   \n",
       "14574  This outdoor scene captures a bright, sunny da...   \n",
       "14575  This image captures an outdoor rooftop terrace...   \n",
       "14576  This outdoor scene depicts a harsh, wintry lan...   \n",
       "14577  This outdoor scene captures a vibrant, sunny g...   \n",
       "14578  The outdoor scene captures a dynamic moment in...   \n",
       "\n",
       "                                    question  question_id  \\\n",
       "0      Please describe this image in detail.            0   \n",
       "1      Please describe this image in detail.            1   \n",
       "2      Please describe this image in detail.            2   \n",
       "3      Please describe this image in detail.            3   \n",
       "4      Please describe this image in detail.            4   \n",
       "...                                      ...          ...   \n",
       "14574  Please describe this image in detail.        14574   \n",
       "14575  Please describe this image in detail.        14575   \n",
       "14576  Please describe this image in detail.        14576   \n",
       "14577  Please describe this image in detail.        14577   \n",
       "14578  Please describe this image in detail.        14578   \n",
       "\n",
       "                                              image_path  \n",
       "0      /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "1      /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "2      /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "3      /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "4      /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "...                                                  ...  \n",
       "14574  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "14575  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "14576  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "14577  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "14578  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "\n",
       "[14579 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92d8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = batch_df[~batch_df[\"local_path\"].isin(poc_df[\"local_path\"].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86245499",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_batch_1_15000_gcp_upload_urs.csv\")\n",
    "prompt_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/llava_gen_des_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7391715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_path</th>\n",
       "      <th>gcs_uri</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "      <td>gs://train_data_vision_1/coco_batch_1_15000/CO...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "      <td>gs://train_data_vision_1/coco_batch_1_15000/CO...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          local_path  \\\n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...   \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...   \n",
       "\n",
       "                                             gcs_uri  error  \n",
       "0  gs://train_data_vision_1/coco_batch_1_15000/CO...    NaN  \n",
       "1  gs://train_data_vision_1/coco_batch_1_15000/CO...    NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76f7260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000078572.jpg</td>\n",
       "      <td>This outdoor scene captures a bustling street ...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>0</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000401963.jpg</td>\n",
       "      <td>The image captures an outdoor scene under brig...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>1</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id  \\\n",
       "0  COCO_train2014_000000078572.jpg   \n",
       "1  COCO_train2014_000000401963.jpg   \n",
       "\n",
       "                                              answer  \\\n",
       "0  This outdoor scene captures a bustling street ...   \n",
       "1  The image captures an outdoor scene under brig...   \n",
       "\n",
       "                                question  question_id  \\\n",
       "0  Please describe this image in detail.            0   \n",
       "1  Please describe this image in detail.            1   \n",
       "\n",
       "                                          image_path  \n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e1cf0",
   "metadata": {},
   "source": [
    "# batch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt(question, ans):\n",
    "#   prompt = f\"\"\"\n",
    "# You are given an **image** and its **caption** describing the visual scene.\n",
    "# Your job is to **analyze the caption** and extract all **meaningful, semantically important words** that can be visually grounded in the image.\n",
    "# Each extracted word must be categorized into one of the following classes:\n",
    "\n",
    "# * **objects** (car, tree, person, lamp)\n",
    "# * **attributes** (red, shiny, tall)\n",
    "# * **relations** (on, under, behind, next_to)\n",
    "# * **actions** (running, holding, sitting)\n",
    "# * **count** (two, many, three)\n",
    "# * **scene/context** (beach, kitchen, street)\n",
    "# * **decision tokens** (yes, no, true, false, present, absent, exist, not, visible, correct)\n",
    "\n",
    "# For every detected word, provide a structured JSON entry with its **category, word, related object IDs, and bounding box** coordinates.\n",
    "# If the word refers to a hallucinated (nonexistent) visual concept, set its bounding box as `[0, 0, 0, 0]`.\n",
    "\n",
    "# Note: always use image scale as 1000 * 1000 for bounding box coordinates.\n",
    "# ---\n",
    "\n",
    "# ### ðŸ§¾ **Expected Output Format**\n",
    "\n",
    "# ```json\n",
    "# {{\n",
    "#   \"objects\": [\n",
    "#     {{\"id\": 1, \"word\": \"dog\", \"bounding_box\": [120, 150, 260, 310]}},\n",
    "#     {{\"id\": 2, \"word\": \"ball\", \"bounding_box\": [400, 230, 460, 280]}}\n",
    "#   ],\n",
    "#   \"attributes\": [\n",
    "#     {{\"word\": \"brown\", \"objects_involved\": [1]}},\n",
    "#     {{\"word\": \"round\", \"objects_involved\": [2]}}\n",
    "#   ],\n",
    "#   \"relations\": [\n",
    "#     {{\"word\": \"holding\", \"objects_involved\": [1, 2]}},\n",
    "#    {{\"word\": \"on\", \"objects_involved\": [1, 2]}}\n",
    "#   ],\n",
    "#   \"count\": [\n",
    "#     {{\"word\": \"two\", \"objects_involved\": [1, 2]}}\n",
    "#   ],\n",
    "#   \"scene\": [\n",
    "#     {{\"word\": \"park\", \"bounding_box\": [0, 0, 640, 480]}}\n",
    "#   ],\n",
    "#   \"decision_tokens\": [\n",
    "#    {{\"word\": \"visible\", \"objects_involved\": [1]}}\n",
    "#   ]\n",
    "# }}\n",
    "# ```\n",
    "# Inputs:\n",
    "# <attached image>\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Caption: {ans}\n",
    "\n",
    "# ---\n",
    "# \"\"\"\n",
    "#   return prompt\n",
    "\n",
    "all_res = []\n",
    "for inx, row in prompt_df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    ans  = row[\"answer\"]\n",
    "    PROMPT = get_prompt(question, ans)\n",
    "    img_path = row[\"image_path\"]\n",
    "    uri = final_df[final_df[\"local_path\"] == img_path][\"gcs_uri\"].iloc[0]\n",
    "    \n",
    "    res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "                      \"generationConfig\": {\"temperature\": 0, \"topP\": 1, \"maxOutputTokens\": 5000,\"thinking_config\":{\"thinking_budget\":1000}}}}\n",
    "    all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9e86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PROMPT = \"\"\"\n",
    "# You are a specialist in rich and precise scene understanding.\n",
    "# Given an input image, generate a comprehensive, contextually aware, and fluent description that captures all key visual elements, their relationships, emotions, and possible context or story.\n",
    "\n",
    "# Your description should go beyond short captions â€” it must resemble a paragraph of visual storytelling that includes:\n",
    "\n",
    "# Scene type: indoor/outdoor, environment, lighting, time of day\n",
    "# Objects and entities: names, counts, shapes, colors, materials\n",
    "# Actions and interactions: what the people or objects are doing\n",
    "# Spatial layout: foreground, background, relative positions\n",
    "# Emotions or atmosphere: tone, mood, aesthetics\n",
    "# Possible context: what might be happening or implied by the scene\n",
    "\n",
    "# Avoid generic or repetitive statements. Be vivid, factual, and coherent. Use natural language instead of bullet points.\n",
    "\n",
    "# Output JSON format:\n",
    "# {image_description: <full attached image description>}\n",
    "# \"\"\"\n",
    "\n",
    "# all_res = []\n",
    "# for uri in df[\"gcs_uri\"].tolist():\n",
    "#     res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "#                       \"generationConfig\": {\"temperature\": 0.6, \"topP\": 1, \"maxOutputTokens\": 1000,\"thinking_config\":{\"thinking_budget\":0}}}}\n",
    "#     all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a069954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(question, ans, img_id):\n",
    "  prompt = f\"\"\"\n",
    "You are given an **image** and **questionâ€“answer (QA) pair** related to that image.\n",
    "Your job is to analyze QA pair separately and return **two lists of single words (no phrases)**:\n",
    "\n",
    "1. **`candidates`** â€” all meaningful words that are semantically important for hallucination detection, belonging to categories like:\n",
    "\n",
    "   * **objects** (car, tree, person, lamp),\n",
    "   * **attributes** (red, shiny, tall),\n",
    "   * **relations** (on, under, behind, next_to),\n",
    "   * **actions** (running, holding, sitting),\n",
    "   * **count** (two, many, three),\n",
    "   * **scene/context** (beach, kitchen, street).\n",
    "   * **Decision tokens** yes, no, true, false, present, absent, exist, not, visible, correct\n",
    "\n",
    "   Include only **specific, content-bearing words** that could be visually verifiable or falsifiable from the image.\n",
    "   Do **not** include stopwords, determiners, or abstract terms.\n",
    "\n",
    "2. **`hallucinated_words`** â€” a subset of the above list that are **not visually supported** or are **contradicted** by the image content (i.e., hallucinated terms).\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "<attached image> {img_id}\n",
    "\n",
    "qa_pair:\n",
    "Question: {question}\n",
    "Answer: {ans}\n",
    "\n",
    "\n",
    "**Output format (strict JSON per QA pair):**\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"candidates\": [\"word1\", \"word2\", ...],\n",
    "    \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "  }}\n",
    "```\n",
    "Note: extract \"candidates\" and \"hallucinated_words\" from the Answer strictly not from the Question\n",
    "\n",
    "Ensure both lists contain **only lowercase single words** and are **deduplicated**.\n",
    "Focus on **important, visually grounded words** â€” not all tokens.\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "  return prompt\n",
    "  \n",
    "all_res = []\n",
    "for inx, row in prompt_df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    ans  = row[\"answer\"]\n",
    "    PROMPT = get_prompt(question, ans, row[\"image_id\"])\n",
    "    img_path = row[\"image_path\"]\n",
    "    uri = final_df[final_df[\"local_path\"] == img_path][\"gcs_uri\"].iloc[0]\n",
    "  \n",
    "    res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "                      \"generationConfig\": {\"temperature\": 0, \"topP\": 1, \"maxOutputTokens\": 2500,\"thinking_config\":{\"thinking_budget\":1000}}}}\n",
    "    all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c687a209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://train_data_vision_1/coco_batch_1_15000/COCO_train2014_000000419627.jpg'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8b909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt(question, ans_1, ans_2):\n",
    "#   prompt = f\"\"\"\n",
    "# You are given an **image** and **two captions (descriptions)** related to that image.\n",
    "# Your job is to analyze each caption separately and return **two lists of single words (no phrases)**:\n",
    "\n",
    "# 1. **`hallucination_candidates`** â€” all meaningful words that are semantically important for hallucination detection, belonging to categories like:\n",
    "\n",
    "#    * **objects** (car, tree, person, lamp),\n",
    "#    * **attributes** (red, shiny, tall),\n",
    "#    * **relations** (on, under, behind, next_to),\n",
    "#    * **actions** (running, holding, sitting),\n",
    "#    * **count** (two, many, three),\n",
    "#    * **scene/context** (beach, kitchen, street).\n",
    "#    * **Decision tokens** yes, no, true, false, present, absent, exist, not, visible, correct\n",
    "\n",
    "#    Include only **specific, content-bearing words** that could be visually verifiable or falsifiable from the image.\n",
    "#    Do **not** include stopwords, determiners, or abstract terms.\n",
    "\n",
    "# 2. **`hallucinated_words`** â€” a subset of the above list that are **not visually supported** or are **contradicted** by the image content (i.e., hallucinated terms).\n",
    "\n",
    "# **Inputs:**\n",
    "\n",
    "# <attached image>\n",
    "\n",
    "# caption_pair_1:\n",
    "# Question: {question}\n",
    "# Caption: {ans_1}\n",
    "\n",
    "# caption_pair_2:\n",
    "# Question: {question}\n",
    "# Caption: {ans_2}\n",
    "\n",
    "\n",
    "# **Output format (strict JSON per caption pair):**\n",
    "\n",
    "# ```json\n",
    "# {{\n",
    "#   \"caption_pair_1\": {{\n",
    "#     \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "#     \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "#   }},\n",
    "#   \"caption_pair_2\": {{\n",
    "#     \"hallucination_candidates\": [\"word1\", \"word2\", ...],\n",
    "#     \"hallucinated_words\": [\"wordX\", \"wordY\", ...]\n",
    "#   }}\n",
    "# }}\n",
    "# ```\n",
    "# Note: extract \"hallucination_candidates\" and \"hallucinated_words\" from the Answer strictly not from the Question\n",
    "\n",
    "# Ensure both lists contain **only lowercase single words** and are **deduplicated**.\n",
    "# Focus on **important, visually grounded words** â€” not all tokens.\n",
    "\n",
    "# ---\n",
    "# \"\"\"\n",
    "#   return prompt\n",
    "\n",
    "# all_res = []\n",
    "# for inx, row in prompt_df.iterrows():\n",
    "#     question = row[\"prompt\"].replace(\"<image>\", \"\")\n",
    "#     nh_ans  = row[\"source_text\"]\n",
    "#     h_ans = row[\"hallucinated_text\"]\n",
    "#     PROMPT = get_prompt(question, nh_ans, h_ans)\n",
    "#     img_path = f\"/Data2/Arun-UAV/NLP/vision_halu/visual_genome/target_images/{row['image_id']}.jpg\"\n",
    "#     uri = df[df[\"local_path\"] == img_path][\"gcs_uri\"].iloc[0]\n",
    "\n",
    "#     res = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": PROMPT}, {\"fileData\": {\"fileUri\": uri, \"mimeType\": \"image/jpeg\"}}]}], \n",
    "#                       \"generationConfig\": {\"temperature\": 0, \"topP\": 1, \"maxOutputTokens\": 2000,\"thinking_config\":{\"thinking_budget\":1000}}}}\n",
    "#     all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e0a8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res)\n",
    "df.to_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab913889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29222894",
   "metadata": {},
   "source": [
    "# Uploading files to gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d038d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(local_path: str, gcs_uri: str):\n",
    "    \"\"\"\n",
    "    Upload a local file to a target GCS URI.\n",
    "\n",
    "    Args:\n",
    "        local_path (str): Local file path to upload.\n",
    "        gcs_uri (str): Target GCS URI like 'gs://my-bucket/path/to/upload.txt'\n",
    "        service_account_path (str): Path to GCP service account JSON.\n",
    "    \"\"\"\n",
    "    if not gcs_uri.startswith(\"gs://\"):\n",
    "        raise ValueError(\"Invalid GCS URI. Must start with gs://\")\n",
    "\n",
    "    parts = gcs_uri[5:].split(\"/\", 1)\n",
    "    bucket_name = parts[0]\n",
    "    blob_name = parts[1]\n",
    "\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(local_path)\n",
    "\n",
    "    print(f\"âœ… Uploaded {local_path} â†’ {gcs_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cf3a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Uploaded /Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl â†’ gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\n"
     ]
    }
   ],
   "source": [
    "upload_to_gcs(local_path=\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_batch.jsonl\", gcs_uri = \"gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf87eca",
   "metadata": {},
   "source": [
    "# start batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9efc5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: projects/358874265041/locations/us-central1/batchPredictionJobs/623136068874534912\n",
      "Job state: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import CreateBatchJobConfig, JobState, HttpOptions\n",
    "output_uri = \"gs://train_data_vision_1/gemini_batch_info/\"\n",
    "\n",
    "# See the documentation: https://googleapis.github.io/python-genai/genai.html#genai.batches.Batches.create\n",
    "job = gen_client.batches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    src=\"gs://train_data_vision_1/gemini_batch_info/gemini_batch.jsonl\",\n",
    "    config=CreateBatchJobConfig(dest=output_uri),\n",
    ")\n",
    "print(f\"Job name: {job.name}\")\n",
    "print(f\"Job state: {job.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9126c060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_info = gen_client.batches.get(name=job.name)\n",
    "job_info.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf8435",
   "metadata": {},
   "source": [
    "# download batch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c16b2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gcs(gcs_uri: str, local_path: str):\n",
    "    \"\"\"\n",
    "    Download a file from GCS based on its gs:// URI.\n",
    "\n",
    "    Args:\n",
    "        gcs_uri (str): GCS URI like 'gs://my-bucket/path/to/file.txt'\n",
    "        local_path (str): Path to store the downloaded file locally.\n",
    "    \"\"\"\n",
    "    # Parse bucket and blob name\n",
    "    if not gcs_uri.startswith(\"gs://\"):\n",
    "        raise ValueError(\"Invalid GCS URI. Must start with gs://\")\n",
    "\n",
    "    parts = gcs_uri[5:].split(\"/\", 1)\n",
    "    bucket_name = parts[0]\n",
    "    blob_name = parts[1]\n",
    "    \n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    blob.download_to_filename(local_path)\n",
    "\n",
    "    print(f\"âœ… Downloaded {gcs_uri} â†’ {local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7622509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Downloaded gs://train_data_vision_1/gemini_batch_info/prediction-model-2025-10-30T09:55:02.630902Z/predictions.jsonl â†’ /Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\n"
     ]
    }
   ],
   "source": [
    "download_from_gcs(gcs_uri=\"gs://train_data_vision_1/gemini_batch_info/prediction-model-2025-10-30T09:55:02.630902Z/predictions.jsonl\", local_path =\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a24bb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.read_json(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/gemini_btach_res.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b539e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_word_2_bbox(target_df, objects_df):\n",
    "#     bbox_map = {}\n",
    "#     for row in target_df.iterrows():\n",
    "#         word = row[1][\"word\"]\n",
    "#         obj_ids = row[1][\"objects_involved\"]\n",
    "#         bbox = objects_df[objects_df[\"id\"].isin(obj_ids)][\"bounding_box\"].to_list()\n",
    "#         bbox_map[word] = bbox\n",
    "#     return bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9787cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COCO_train2014_000000321493.jpg'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "348f4d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>COCO_train2014_000000321493.jpg</td>\n",
       "      <td>This outdoor scene captures a bright, clear da...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>655</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            image_id  \\\n",
       "655  COCO_train2014_000000321493.jpg   \n",
       "\n",
       "                                                answer  \\\n",
       "655  This outdoor scene captures a bright, clear da...   \n",
       "\n",
       "                                  question  question_id  \\\n",
       "655  Please describe this image in detail.          655   \n",
       "\n",
       "                                            image_path  \n",
       "655  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df[prompt_df[\"image_id\"] == img_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0270f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14579it [00:21, 669.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_res = []\n",
    "error = 0\n",
    "for inx, row in tqdm(pred_data.iterrows()):\n",
    "    try:\n",
    "        img_name = row[\"request\"][\"contents\"][0][\"parts\"][1][\"fileData\"][\"fileUri\"].split(\"/\")[-1]\n",
    "        image_path = \"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/poc_5000_coco_images/\" + img_name\n",
    "        req_info = prompt_df[prompt_df[\"image_id\"] == img_name]\n",
    "        question = req_info[\"question\"].iloc[0]\n",
    "        ans  = req_info[\"answer\"].iloc[0]\n",
    "        res = eval(row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].replace(\"json\", \"\").strip(\"```\"))\n",
    "        candidates = res[\"candidates\"]\n",
    "        hallucinated_words = res[\"hallucinated_words\"]\n",
    "\n",
    "\n",
    "        all_res.append({\"image_id\": img_name, \"question\": question, \"answer\": ans, \"image_path\": image_path, \"candidates\": candidates, \"hallucination_candidates\": hallucinated_words})\n",
    "\n",
    "    except Exception as e:\n",
    "        error += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce48dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_res_df = pd.DataFrame(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40ef4c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_path</th>\n",
       "      <th>candidates</th>\n",
       "      <th>hallucination_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000321493.jpg</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>This outdoor scene captures a bright, clear da...</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "      <td>[outdoor, bright, clear, day, sprawling, dry, ...</td>\n",
       "      <td>[concentration, effort, overhead, behind, view...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000405541.jpg</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>The indoor scene captures a striking Siamese c...</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "      <td>[indoor, siamese, cat, snowshoe, lounging, woo...</td>\n",
       "      <td>[caramel-colored, fluffy, pink, art, supplies,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id                               question  \\\n",
       "0  COCO_train2014_000000321493.jpg  Please describe this image in detail.   \n",
       "1  COCO_train2014_000000405541.jpg  Please describe this image in detail.   \n",
       "\n",
       "                                              answer  \\\n",
       "0  This outdoor scene captures a bright, clear da...   \n",
       "1  The indoor scene captures a striking Siamese c...   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...   \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...   \n",
       "\n",
       "                                          candidates  \\\n",
       "0  [outdoor, bright, clear, day, sprawling, dry, ...   \n",
       "1  [indoor, siamese, cat, snowshoe, lounging, woo...   \n",
       "\n",
       "                            hallucination_candidates  \n",
       "0  [concentration, effort, overhead, behind, view...  \n",
       "1  [caramel-colored, fluffy, pink, art, supplies,...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab394aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_res_df.to_csv(\"/Data2/Arun-UAV/NLP/vision_halu/hal_detection_head_train_datasets/coco/gemini_labeld_15k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae50e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df = pd.merge(clean_res_df, prompt_df, on = \"image_id\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "70534e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>word_2_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000507721.jpg</td>\n",
       "      <td>{'grass': [0, 400, 1000, 1000], 'structure': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000560217.jpg</td>\n",
       "      <td>{'cat': [150, 180, 850, 800], 'film stock labe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id  \\\n",
       "0  COCO_train2014_000000507721.jpg   \n",
       "1  COCO_train2014_000000560217.jpg   \n",
       "\n",
       "                                         word_2_bbox  \n",
       "0  {'grass': [0, 400, 1000, 1000], 'structure': [...  \n",
       "1  {'cat': [150, 180, 850, 800], 'film stock labe...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33e1026b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8803</th>\n",
       "      <td>COCO_train2014_000000559900.jpg</td>\n",
       "      <td>This indoor scene captures two young boys enjo...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>8803</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>COCO_train2014_000000463308.jpg</td>\n",
       "      <td>This outdoor scene captures a dynamic moment d...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>8660</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_id  \\\n",
       "8803  COCO_train2014_000000559900.jpg   \n",
       "8660  COCO_train2014_000000463308.jpg   \n",
       "\n",
       "                                                 answer  \\\n",
       "8803  This indoor scene captures two young boys enjo...   \n",
       "8660  This outdoor scene captures a dynamic moment d...   \n",
       "\n",
       "                                   question  question_id  \\\n",
       "8803  Please describe this image in detail.         8803   \n",
       "8660  Please describe this image in detail.         8660   \n",
       "\n",
       "                                             image_path  \n",
       "8803  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "8660  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df22d5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>word_2_bbox</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000507721.jpg</td>\n",
       "      <td>{'grass': [0, 400, 1000, 1000], 'structure': [...</td>\n",
       "      <td>This outdoor scene captures a rustic, rural la...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>5616</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000560217.jpg</td>\n",
       "      <td>{'cat': [150, 180, 850, 800], 'film stock labe...</td>\n",
       "      <td>This outdoor, dimly lit scene, captured likely...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>1966</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id  \\\n",
       "0  COCO_train2014_000000507721.jpg   \n",
       "1  COCO_train2014_000000560217.jpg   \n",
       "\n",
       "                                         word_2_bbox  \\\n",
       "0  {'grass': [0, 400, 1000, 1000], 'structure': [...   \n",
       "1  {'cat': [150, 180, 850, 800], 'film stock labe...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  This outdoor scene captures a rustic, rural la...   \n",
       "1  This outdoor, dimly lit scene, captured likely...   \n",
       "\n",
       "                                question  question_id  \\\n",
       "0  Please describe this image in detail.         5616   \n",
       "1  Please describe this image in detail.         1966   \n",
       "\n",
       "                                          image_path  \n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4e2bded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/evidence_head_train_datasets/coco_long_captions/coco_img_des_10k_bb_annot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "21243266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>word_2_bbox</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000507721.jpg</td>\n",
       "      <td>{'grass': [0, 400, 1000, 1000], 'structure': [...</td>\n",
       "      <td>This outdoor scene captures a rustic, rural la...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>5616</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000560217.jpg</td>\n",
       "      <td>{'cat': [150, 180, 850, 800], 'film stock labe...</td>\n",
       "      <td>This outdoor, dimly lit scene, captured likely...</td>\n",
       "      <td>Please describe this image in detail.</td>\n",
       "      <td>1966</td>\n",
       "      <td>/Data2/Arun-UAV/NLP/vision_halu/train_datasets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image_id  \\\n",
       "0  COCO_train2014_000000507721.jpg   \n",
       "1  COCO_train2014_000000560217.jpg   \n",
       "\n",
       "                                         word_2_bbox  \\\n",
       "0  {'grass': [0, 400, 1000, 1000], 'structure': [...   \n",
       "1  {'cat': [150, 180, 850, 800], 'film stock labe...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  This outdoor scene captures a rustic, rural la...   \n",
       "1  This outdoor, dimly lit scene, captured likely...   \n",
       "\n",
       "                                question  question_id  \\\n",
       "0  Please describe this image in detail.         5616   \n",
       "1  Please describe this image in detail.         1966   \n",
       "\n",
       "                                          image_path  \n",
       "0  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  \n",
       "1  /Data2/Arun-UAV/NLP/vision_halu/train_datasets...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175add22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d15f24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_qa_pairs(text: str):\n",
    "    \"\"\"\n",
    "    Extracts qa_pair_1 and qa_pair_2 (Question and Answer) from multiple prompts\n",
    "    having the same structure.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text containing multiple prompt blocks.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of dictionaries, each containing qa_pair_1 and qa_pair_2 with Q/A.\n",
    "    \"\"\"\n",
    "    # Pattern to match both caption_pair_1 and caption_pair_2 blocks\n",
    "    pattern = re.compile(\n",
    "        r'caption_pair_1:\\s*Question:\\s*(?P<q1>.*?)\\s*Caption:\\s*(?P<a1>.*?)\\s*'\n",
    "        r'caption_pair_2:\\s*Question:\\s*(?P<q2>.*?)\\s*Caption:\\s*(?P<a2>.*?)(?=\\n\\s*\\n|$)',\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for match in pattern.finditer(text):\n",
    "        qa_dict = {\n",
    "            \"caption_pair_1\": {\n",
    "                \"Question\": match.group(\"q1\").strip(),\n",
    "                \"Caption\": match.group(\"a1\").strip()\n",
    "            },\n",
    "            \"caption_pair_2\": {\n",
    "                \"Question\": match.group(\"q2\").strip(),\n",
    "                \"Caption\": match.group(\"a2\").strip()\n",
    "            }\n",
    "        }\n",
    "        results.append(qa_dict)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6c523ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "error = 0\n",
    "for inx, row in pred_data.iterrows():\n",
    "    try:\n",
    "        img_name = row[\"request\"][\"contents\"][0][\"parts\"][1][\"fileData\"][\"fileUri\"].split(\"/\")[-1]\n",
    "        qa = extract_qa_pairs(row[\"request\"][\"contents\"][0][\"parts\"][0][\"text\"].split(\"<attached image>\")[-1].split(\"**Output format\")[0])\n",
    "\n",
    "        question = qa[0][\"caption_pair_1\"]['Question']\n",
    "        answer_1 = qa[0][\"caption_pair_1\"]['Caption']\n",
    "        answer_2 = qa[0][\"caption_pair_2\"]['Caption']\n",
    "\n",
    "        img_dec = row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        c_img_dec = eval(img_dec.replace(\"json\", \"\").strip(\"```\"))\n",
    "\n",
    "        all_res.append([img_name, question, answer_1,  c_img_dec[\"caption_pair_1\"][\"hallucination_candidates\"], c_img_dec[\"caption_pair_1\"][\"hallucinated_words\"]])\n",
    "        all_res.append([img_name, question, answer_2,  c_img_dec[\"caption_pair_2\"][\"hallucination_candidates\"], c_img_dec[\"caption_pair_2\"][\"hallucinated_words\"]])\n",
    "    except Exception as e:\n",
    "        error += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0368eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84ce5e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27986, 5)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4fea5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns = [\"image_path\", \"question\", \"answer\", \"candidates\", \"hallucination_candidates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76122e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"/Data2/Arun-UAV/NLP/vision_halu/haloc/haloc_extension/caption/gemini_labeled_28k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a1e4a39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>candidates</th>\n",
       "      <th>hallucination_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2381430.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see one woman is holdin...</td>\n",
       "      <td>[one, woman, holding, bat, playing, tennis, ba...</td>\n",
       "      <td>[bat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381430.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see one woman is holdin...</td>\n",
       "      <td>[one, woman, holding, bat, playing, tennis, fr...</td>\n",
       "      <td>[bat, frisbee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2378712.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see a bus on road and a...</td>\n",
       "      <td>[bus, road, footpath, statue, person, building...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2378712.jpg</td>\n",
       "      <td>What do you think is going on in this snapshot?</td>\n",
       "      <td>In this picture we can see a bus on road and a...</td>\n",
       "      <td>[bus, road, footpath, statue, person, building...</td>\n",
       "      <td>[trains]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2379616.jpg</td>\n",
       "      <td>Describe the following image.</td>\n",
       "      <td>There is a person standing on a bicycle and th...</td>\n",
       "      <td>[person, standing, bicycle, car, beside, sitti...</td>\n",
       "      <td>[standing, sitting, bike, left, corner, few, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>2398155.jpg</td>\n",
       "      <td>What do you see happening in this image?</td>\n",
       "      <td>An older woman in a black, white, and gray shi...</td>\n",
       "      <td>[older, woman, black, white, gray, shirt, pant...</td>\n",
       "      <td>[gray, apron, five, stove]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27982</th>\n",
       "      <td>2347016.jpg</td>\n",
       "      <td>Write a detailed description of the given image.</td>\n",
       "      <td>In this picture we can see a black color cat s...</td>\n",
       "      <td>[black, cat, sitting, laptop, on, book, pencil...</td>\n",
       "      <td>[cd, remote]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27983</th>\n",
       "      <td>2347016.jpg</td>\n",
       "      <td>Write a detailed description of the given image.</td>\n",
       "      <td>In this picture we can see a black color cat s...</td>\n",
       "      <td>[black, cat, sitting, laptop, on, book, pencil...</td>\n",
       "      <td>[left, cd, remote, cup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27984</th>\n",
       "      <td>2316274.jpg</td>\n",
       "      <td>Can you elaborate on the elements of the pictu...</td>\n",
       "      <td>In this image there is a dog standing in a roo...</td>\n",
       "      <td>[dog, standing, room, carpet, books, tables, c...</td>\n",
       "      <td>[standing, toys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27985</th>\n",
       "      <td>2316274.jpg</td>\n",
       "      <td>Can you elaborate on the elements of the pictu...</td>\n",
       "      <td>In this image there is a dog standing in a roo...</td>\n",
       "      <td>[dog, standing, room, carpet, books, tables, c...</td>\n",
       "      <td>[standing, toys, bed, frisbees, scattered]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27986 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_path                                           question  \\\n",
       "0      2381430.jpg    What do you think is going on in this snapshot?   \n",
       "1      2381430.jpg    What do you think is going on in this snapshot?   \n",
       "2      2378712.jpg    What do you think is going on in this snapshot?   \n",
       "3      2378712.jpg    What do you think is going on in this snapshot?   \n",
       "4      2379616.jpg                      Describe the following image.   \n",
       "...            ...                                                ...   \n",
       "27981  2398155.jpg           What do you see happening in this image?   \n",
       "27982  2347016.jpg   Write a detailed description of the given image.   \n",
       "27983  2347016.jpg   Write a detailed description of the given image.   \n",
       "27984  2316274.jpg  Can you elaborate on the elements of the pictu...   \n",
       "27985  2316274.jpg  Can you elaborate on the elements of the pictu...   \n",
       "\n",
       "                                                  answer  \\\n",
       "0      In this picture we can see one woman is holdin...   \n",
       "1      In this picture we can see one woman is holdin...   \n",
       "2      In this picture we can see a bus on road and a...   \n",
       "3      In this picture we can see a bus on road and a...   \n",
       "4      There is a person standing on a bicycle and th...   \n",
       "...                                                  ...   \n",
       "27981  An older woman in a black, white, and gray shi...   \n",
       "27982  In this picture we can see a black color cat s...   \n",
       "27983  In this picture we can see a black color cat s...   \n",
       "27984  In this image there is a dog standing in a roo...   \n",
       "27985  In this image there is a dog standing in a roo...   \n",
       "\n",
       "                                              candidates  \\\n",
       "0      [one, woman, holding, bat, playing, tennis, ba...   \n",
       "1      [one, woman, holding, bat, playing, tennis, fr...   \n",
       "2      [bus, road, footpath, statue, person, building...   \n",
       "3      [bus, road, footpath, statue, person, building...   \n",
       "4      [person, standing, bicycle, car, beside, sitti...   \n",
       "...                                                  ...   \n",
       "27981  [older, woman, black, white, gray, shirt, pant...   \n",
       "27982  [black, cat, sitting, laptop, on, book, pencil...   \n",
       "27983  [black, cat, sitting, laptop, on, book, pencil...   \n",
       "27984  [dog, standing, room, carpet, books, tables, c...   \n",
       "27985  [dog, standing, room, carpet, books, tables, c...   \n",
       "\n",
       "                                hallucination_candidates  \n",
       "0                                                  [bat]  \n",
       "1                                         [bat, frisbee]  \n",
       "2                                                     []  \n",
       "3                                               [trains]  \n",
       "4      [standing, sitting, bike, left, corner, few, r...  \n",
       "...                                                  ...  \n",
       "27981                         [gray, apron, five, stove]  \n",
       "27982                                       [cd, remote]  \n",
       "27983                            [left, cd, remote, cup]  \n",
       "27984                                   [standing, toys]  \n",
       "27985         [standing, toys, bed, frisbees, scattered]  \n",
       "\n",
       "[27986 rows x 5 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d45f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a473d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []\n",
    "for inx, row in pred_data.iterrows():\n",
    "    try:\n",
    "        img_name = row[\"request\"][\"contents\"][0][\"parts\"][1][\"fileData\"][\"fileUri\"].split(\"/\")[-1]\n",
    "        img_dec = row[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        c_img_dec = eval(img_dec.replace(\"json\", \"\").strip(\"```\"))[\"image_description\"]\n",
    "        all_res.append({\"image\": img_name, \"description\": c_img_dec})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {inx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cd58247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eb243a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000467172.jpg</td>\n",
       "      <td>The indoor scene captures two domestic cats ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000328023.jpg</td>\n",
       "      <td>This outdoor scene captures a lively moment in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image  \\\n",
       "0  COCO_train2014_000000467172.jpg   \n",
       "1  COCO_train2014_000000328023.jpg   \n",
       "\n",
       "                                         description  \n",
       "0  The indoor scene captures two domestic cats ex...  \n",
       "1  This outdoor scene captures a lively moment in...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ebb6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_poc_df  = pd.read_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_5000_train_with_gemini_des.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef526b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_5000_train_with_gemini_des.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b50f8312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_train2014_000000078572.jpg</td>\n",
       "      <td>This outdoor scene captures a bustling street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000401963.jpg</td>\n",
       "      <td>The image captures an outdoor scene under brig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image  \\\n",
       "0  COCO_train2014_000000078572.jpg   \n",
       "1  COCO_train2014_000000401963.jpg   \n",
       "\n",
       "                                         description  \n",
       "0  This outdoor scene captures a bustling street ...  \n",
       "1  The image captures an outdoor scene under brig...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_poc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ed88430",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_15k = pd.concat([old_poc_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f44e69ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14579"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_15k[\"image\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7913014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_15k.to_csv(\"/Data2/Arun-UAV/NLP/vision_halu/train_datasets/coco_batch_1_15000_train_with_gemini_des.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0d87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
